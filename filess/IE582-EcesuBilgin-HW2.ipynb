{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7decf4d",
   "metadata": {},
   "source": [
    "I couldn't find the according data that have seperately labeled test data, so I split the data into train and test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457e4d7f",
   "metadata": {},
   "source": [
    "### 1. Dataset: German Statlog Credit Data\n",
    "My first data is German Statlog Credit Data. It is a binary classification (\"good\" or \"bad\") to a regression problem. The data size is 1000x21. I directly imported the data from UCI.\n",
    "Link to data: https://archive.ics.uci.edu/dataset/144/statlog+german+credit+data\n",
    "\n",
    "**Variable describtion from the website**\n",
    "Attribute 1:  (qualitative)      \n",
    " Status of existing checking account\n",
    "             A11 :      ... <    0 DM\n",
    "\t       A12 : 0 <= ... <  200 DM\n",
    "\t       A13 :      ... >= 200 DM / salary assignments for at least 1 year\n",
    "               A14 : no checking account\n",
    "\n",
    "Attribute 2:  (numerical)\n",
    "\t      Duration in month\n",
    "\n",
    "Attribute 3:  (qualitative)\n",
    "\t      Credit history\n",
    "\t      A30 : no credits taken/ all credits paid back duly\n",
    "              A31 : all credits at this bank paid back duly\n",
    "\t      A32 : existing credits paid back duly till now\n",
    "              A33 : delay in paying off in the past\n",
    "\t      A34 : critical account/  other credits existing (not at this bank)\n",
    "\n",
    "Attribute 4:  (qualitative)\n",
    "\t      Purpose\n",
    "\t      A40 : car (new)\n",
    "\t      A41 : car (used)\n",
    "\t      A42 : furniture/equipment\n",
    "\t      A43 : radio/television\n",
    "\t      A44 : domestic appliances\n",
    "\t      A45 : repairs\n",
    "\t      A46 : education\n",
    "\t      A47 : (vacation - does not exist?)\n",
    "\t      A48 : retraining\n",
    "\t      A49 : business\n",
    "\t      A410 : others\n",
    "\n",
    "Attribute 5:  (numerical)\n",
    "\t      Credit amount\n",
    "\n",
    "Attibute 6:  (qualitative)\n",
    "\t      Savings account/bonds\n",
    "\t      A61 :          ... <  100 DM\n",
    "\t      A62 :   100 <= ... <  500 DM\n",
    "\t      A63 :   500 <= ... < 1000 DM\n",
    "\t      A64 :          .. >= 1000 DM\n",
    "              A65 :   unknown/ no savings account\n",
    "\n",
    "Attribute 7:  (qualitative)\n",
    "\t      Present employment since\n",
    "\t      A71 : unemployed\n",
    "\t      A72 :       ... < 1 year\n",
    "\t      A73 : 1  <= ... < 4 years  \n",
    "\t      A74 : 4  <= ... < 7 years\n",
    "\t      A75 :       .. >= 7 years\n",
    "\n",
    "Attribute 8:  (numerical)\n",
    "\t      Installment rate in percentage of disposable income\n",
    "\n",
    "Attribute 9:  (qualitative)\n",
    "\t      Personal status and sex\n",
    "\t      A91 : male   : divorced/separated\n",
    "\t      A92 : female : divorced/separated/married\n",
    "              A93 : male   : single\n",
    "\t      A94 : male   : married/widowed\n",
    "\t      A95 : female : single\n",
    "\n",
    "Attribute 10: (qualitative)\n",
    "\t      Other debtors / guarantors\n",
    "\t      A101 : none\n",
    "\t      A102 : co-applicant\n",
    "\t      A103 : guarantor\n",
    "\n",
    "Attribute 11: (numerical)\n",
    "\t      Present residence since\n",
    "\n",
    "Attribute 12: (qualitative)\n",
    "\t      Property\n",
    "\t      A121 : real estate\n",
    "\t      A122 : if not A121 : building society savings agreement/ life insurance\n",
    "              A123 : if not A121/A122 : car or other, not in attribute 6\n",
    "\t      A124 : unknown / no property\n",
    "\n",
    "Attribute 13: (numerical)\n",
    "\t      Age in years\n",
    "\n",
    "Attribute 14: (qualitative)\n",
    "\t      Other installment plans \n",
    "\t      A141 : bank\n",
    "\t      A142 : stores\n",
    "\t      A143 : none\n",
    "\n",
    "Attribute 15: (qualitative)\n",
    "\t      Housing\n",
    "\t      A151 : rent\n",
    "\t      A152 : own\n",
    "\t      A153 : for free\n",
    "\n",
    "Attribute 16: (numerical)\n",
    "              Number of existing credits at this bank\n",
    "\n",
    "Attribute 17: (qualitative)\n",
    "\t      Job\n",
    "\t      A171 : unemployed/ unskilled  - non-resident\n",
    "\t      A172 : unskilled - resident\n",
    "\t      A173 : skilled employee / official\n",
    "\t      A174 : management/ self-employed/\n",
    "\t\t     highly qualified employee/ officer\n",
    "\n",
    "Attribute 18: (numerical)\n",
    "\t      Number of people being liable to provide maintenance for\n",
    "\n",
    "Attribute 19: (qualitative)\n",
    "\t      Telephone\n",
    "\t      A191 : none\n",
    "\t      A192 : yes, registered under the customers name\n",
    "\n",
    "Attribute 20: (qualitative)\n",
    "\t      foreign worker\n",
    "\t      A201 : yes\n",
    "\t      A202 : no\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "fa794d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ucimlrepo in c:\\users\\dell\\anaconda3\\lib\\site-packages (0.0.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install ucimlrepo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "7020d881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'uci_id': 144, 'name': 'Statlog (German Credit Data)', 'repository_url': 'https://archive.ics.uci.edu/dataset/144/statlog+german+credit+data', 'data_url': 'https://archive.ics.uci.edu/static/public/144/data.csv', 'abstract': 'This dataset classifies people described by a set of attributes as good or bad credit risks. Comes in two formats (one all numeric). Also comes with a cost matrix', 'area': 'Social Science', 'tasks': ['Classification'], 'characteristics': ['Multivariate'], 'num_instances': 1000, 'num_features': 20, 'feature_types': ['Categorical', 'Integer'], 'demographics': ['Other', 'Marital Status', 'Age', 'Occupation'], 'target_col': ['class'], 'index_col': None, 'has_missing_values': 'no', 'missing_values_symbol': None, 'year_of_dataset_creation': 1994, 'last_updated': 'Thu Aug 10 2023', 'dataset_doi': '10.24432/C5NC77', 'creators': ['Hans Hofmann'], 'intro_paper': None, 'additional_info': {'summary': 'Two datasets are provided.  the original dataset, in the form provided by Prof. Hofmann, contains categorical/symbolic attributes and is in the file \"german.data\".   \\r\\n \\r\\nFor algorithms that need numerical attributes, Strathclyde University produced the file \"german.data-numeric\".  This file has been edited and several indicator variables added to make it suitable for algorithms which cannot cope with categorical variables.   Several attributes that are ordered categorical (such as attribute 17) have been coded as integer.    This was the form used by StatLog.\\r\\n\\r\\nThis dataset requires use of a cost matrix (see below)\\r\\n\\r\\n ..... 1        2\\r\\n----------------------------\\r\\n  1   0        1\\r\\n-----------------------\\r\\n  2   5        0\\r\\n\\r\\n(1 = Good,  2 = Bad)\\r\\n\\r\\nThe rows represent the actual classification and the columns the predicted classification.\\r\\n\\r\\nIt is worse to class a customer as good when they are bad (5), than it is to class a customer as bad when they are good (1).\\r\\n', 'purpose': None, 'funded_by': None, 'instances_represent': None, 'recommended_data_splits': None, 'sensitive_data': None, 'preprocessing_description': None, 'variable_info': 'Attribute 1:  (qualitative)      \\r\\n Status of existing checking account\\r\\n             A11 :      ... <    0 DM\\r\\n\\t       A12 : 0 <= ... <  200 DM\\r\\n\\t       A13 :      ... >= 200 DM / salary assignments for at least 1 year\\r\\n               A14 : no checking account\\r\\n\\r\\nAttribute 2:  (numerical)\\r\\n\\t      Duration in month\\r\\n\\r\\nAttribute 3:  (qualitative)\\r\\n\\t      Credit history\\r\\n\\t      A30 : no credits taken/ all credits paid back duly\\r\\n              A31 : all credits at this bank paid back duly\\r\\n\\t      A32 : existing credits paid back duly till now\\r\\n              A33 : delay in paying off in the past\\r\\n\\t      A34 : critical account/  other credits existing (not at this bank)\\r\\n\\r\\nAttribute 4:  (qualitative)\\r\\n\\t      Purpose\\r\\n\\t      A40 : car (new)\\r\\n\\t      A41 : car (used)\\r\\n\\t      A42 : furniture/equipment\\r\\n\\t      A43 : radio/television\\r\\n\\t      A44 : domestic appliances\\r\\n\\t      A45 : repairs\\r\\n\\t      A46 : education\\r\\n\\t      A47 : (vacation - does not exist?)\\r\\n\\t      A48 : retraining\\r\\n\\t      A49 : business\\r\\n\\t      A410 : others\\r\\n\\r\\nAttribute 5:  (numerical)\\r\\n\\t      Credit amount\\r\\n\\r\\nAttibute 6:  (qualitative)\\r\\n\\t      Savings account/bonds\\r\\n\\t      A61 :          ... <  100 DM\\r\\n\\t      A62 :   100 <= ... <  500 DM\\r\\n\\t      A63 :   500 <= ... < 1000 DM\\r\\n\\t      A64 :          .. >= 1000 DM\\r\\n              A65 :   unknown/ no savings account\\r\\n\\r\\nAttribute 7:  (qualitative)\\r\\n\\t      Present employment since\\r\\n\\t      A71 : unemployed\\r\\n\\t      A72 :       ... < 1 year\\r\\n\\t      A73 : 1  <= ... < 4 years  \\r\\n\\t      A74 : 4  <= ... < 7 years\\r\\n\\t      A75 :       .. >= 7 years\\r\\n\\r\\nAttribute 8:  (numerical)\\r\\n\\t      Installment rate in percentage of disposable income\\r\\n\\r\\nAttribute 9:  (qualitative)\\r\\n\\t      Personal status and sex\\r\\n\\t      A91 : male   : divorced/separated\\r\\n\\t      A92 : female : divorced/separated/married\\r\\n              A93 : male   : single\\r\\n\\t      A94 : male   : married/widowed\\r\\n\\t      A95 : female : single\\r\\n\\r\\nAttribute 10: (qualitative)\\r\\n\\t      Other debtors / guarantors\\r\\n\\t      A101 : none\\r\\n\\t      A102 : co-applicant\\r\\n\\t      A103 : guarantor\\r\\n\\r\\nAttribute 11: (numerical)\\r\\n\\t      Present residence since\\r\\n\\r\\nAttribute 12: (qualitative)\\r\\n\\t      Property\\r\\n\\t      A121 : real estate\\r\\n\\t      A122 : if not A121 : building society savings agreement/ life insurance\\r\\n              A123 : if not A121/A122 : car or other, not in attribute 6\\r\\n\\t      A124 : unknown / no property\\r\\n\\r\\nAttribute 13: (numerical)\\r\\n\\t      Age in years\\r\\n\\r\\nAttribute 14: (qualitative)\\r\\n\\t      Other installment plans \\r\\n\\t      A141 : bank\\r\\n\\t      A142 : stores\\r\\n\\t      A143 : none\\r\\n\\r\\nAttribute 15: (qualitative)\\r\\n\\t      Housing\\r\\n\\t      A151 : rent\\r\\n\\t      A152 : own\\r\\n\\t      A153 : for free\\r\\n\\r\\nAttribute 16: (numerical)\\r\\n              Number of existing credits at this bank\\r\\n\\r\\nAttribute 17: (qualitative)\\r\\n\\t      Job\\r\\n\\t      A171 : unemployed/ unskilled  - non-resident\\r\\n\\t      A172 : unskilled - resident\\r\\n\\t      A173 : skilled employee / official\\r\\n\\t      A174 : management/ self-employed/\\r\\n\\t\\t     highly qualified employee/ officer\\r\\n\\r\\nAttribute 18: (numerical)\\r\\n\\t      Number of people being liable to provide maintenance for\\r\\n\\r\\nAttribute 19: (qualitative)\\r\\n\\t      Telephone\\r\\n\\t      A191 : none\\r\\n\\t      A192 : yes, registered under the customers name\\r\\n\\r\\nAttribute 20: (qualitative)\\r\\n\\t      foreign worker\\r\\n\\t      A201 : yes\\r\\n\\t      A202 : no\\r\\n', 'citation': None}}\n",
      "           name     role         type     demographic  \\\n",
      "0    Attribute1  Feature  Categorical            None   \n",
      "1    Attribute2  Feature      Integer            None   \n",
      "2    Attribute3  Feature  Categorical            None   \n",
      "3    Attribute4  Feature  Categorical            None   \n",
      "4    Attribute5  Feature      Integer            None   \n",
      "5    Attribute6  Feature  Categorical            None   \n",
      "6    Attribute7  Feature  Categorical           Other   \n",
      "7    Attribute8  Feature      Integer            None   \n",
      "8    Attribute9  Feature  Categorical  Marital Status   \n",
      "9   Attribute10  Feature  Categorical            None   \n",
      "10  Attribute11  Feature      Integer            None   \n",
      "11  Attribute12  Feature  Categorical            None   \n",
      "12  Attribute13  Feature      Integer             Age   \n",
      "13  Attribute14  Feature  Categorical            None   \n",
      "14  Attribute15  Feature  Categorical           Other   \n",
      "15  Attribute16  Feature      Integer            None   \n",
      "16  Attribute17  Feature  Categorical      Occupation   \n",
      "17  Attribute18  Feature      Integer            None   \n",
      "18  Attribute19  Feature       Binary            None   \n",
      "19  Attribute20  Feature       Binary           Other   \n",
      "20        class   Target       Binary            None   \n",
      "\n",
      "                                          description   units missing_values  \n",
      "0                 Status of existing checking account    None             no  \n",
      "1                                            Duration  months             no  \n",
      "2                                      Credit history    None             no  \n",
      "3                                             Purpose    None             no  \n",
      "4                                       Credit amount    None             no  \n",
      "5                               Savings account/bonds    None             no  \n",
      "6                            Present employment since    None             no  \n",
      "7   Installment rate in percentage of disposable i...    None             no  \n",
      "8                             Personal status and sex    None             no  \n",
      "9                          Other debtors / guarantors    None             no  \n",
      "10                            Present residence since    None             no  \n",
      "11                                           Property    None             no  \n",
      "12                                                Age   years             no  \n",
      "13                            Other installment plans    None             no  \n",
      "14                                            Housing    None             no  \n",
      "15            Number of existing credits at this bank    None             no  \n",
      "16                                                Job    None             no  \n",
      "17  Number of people being liable to provide maint...    None             no  \n",
      "18                                          Telephone    None             no  \n",
      "19                                     foreign worker    None             no  \n",
      "20                                  1 = Good, 2 = Bad    None             no  \n"
     ]
    }
   ],
   "source": [
    "###GERMAN STATLOG DATA\n",
    "\n",
    "\n",
    "from ucimlrepo import fetch_ucirepo \n",
    "  \n",
    "# fetch dataset \n",
    "statlog_german_credit_data = fetch_ucirepo(id=144) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = statlog_german_credit_data.data.features \n",
    "y = statlog_german_credit_data.data.targets \n",
    "  \n",
    "# metadata \n",
    "print(statlog_german_credit_data.metadata) \n",
    "  \n",
    "# variable information \n",
    "print(statlog_german_credit_data.variables) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb680dd",
   "metadata": {},
   "source": [
    "I processed the categorical columns with one hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "e8d54a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best neighbors for Euclidean distance: 12, Train set accuracy:0.7288, Test Set Accuracy: 0.7350\n",
      "Best neighbors for Manhattan distance: 12,Train set accuracy:0.7288, Test Set Accuracy: 0.7350\n"
     ]
    }
   ],
   "source": [
    "##GERMAN STATLOG NN\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "\n",
    "# cat_columns contains the indices of categorical columns in your dataset\n",
    "cat_columns = [0, 2, 3, 5, 6, 8, 9, 11, 13, 14, 16]  #  categorical columns\n",
    "\n",
    "# Preprocessing pipeline to handle categorical columns\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', categorical_transformer, cat_columns)\n",
    "    ])\n",
    "\n",
    "# Transform your data using the preprocessor\n",
    "X_processed = preprocessor.fit_transform(X)\n",
    "y = y.ravel()\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# List of n_neighbors values to experiment with\n",
    "n_neighbors_values = list(range(1, 36))  # Adjust or expand as needed\n",
    "\n",
    "best_accuracy_euclidean = 0\n",
    "best_accuracy_manhattan = 0\n",
    "best_neighbors_euclidean = 0\n",
    "best_neighbors_manhattan = 0\n",
    "\n",
    "for n_neighbors in n_neighbors_values:\n",
    "    # Create K-Nearest Neighbors classifiers with different distance metrics\n",
    "    euclidean_classifier = KNeighborsClassifier(n_neighbors=n_neighbors, metric='euclidean')\n",
    "    manhattan_classifier = KNeighborsClassifier(n_neighbors=n_neighbors, metric='manhattan')\n",
    "\n",
    "    # Fit and score on the training data using cross-validation\n",
    "    euclidean_scores = cross_val_score(euclidean_classifier, X_train, y_train, cv=5)\n",
    "    manhattan_scores = cross_val_score(manhattan_classifier, X_train, y_train, cv=5)\n",
    "\n",
    "    # Calculate average accuracy across folds\n",
    "    avg_accuracy_euclidean = np.mean(euclidean_scores)\n",
    "    avg_accuracy_manhattan = np.mean(manhattan_scores)\n",
    "\n",
    "    if avg_accuracy_euclidean > best_accuracy_euclidean:\n",
    "        best_accuracy_euclidean = avg_accuracy_euclidean\n",
    "        best_neighbors_euclidean = n_neighbors\n",
    "\n",
    "    if avg_accuracy_manhattan > best_accuracy_manhattan:\n",
    "        best_accuracy_manhattan = avg_accuracy_manhattan\n",
    "        best_neighbors_manhattan = n_neighbors\n",
    "\n",
    "# Fit the best models on the entire training set\n",
    "best_euclidean_classifier = KNeighborsClassifier(n_neighbors=best_neighbors_euclidean, metric='euclidean')\n",
    "best_manhattan_classifier = KNeighborsClassifier(n_neighbors=best_neighbors_manhattan, metric='manhattan')\n",
    "\n",
    "best_euclidean_classifier.fit(X_train, y_train)\n",
    "best_manhattan_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_accuracy_euclidean = best_euclidean_classifier.score(X_test, y_test)\n",
    "test_accuracy_manhattan = best_manhattan_classifier.score(X_test, y_test)\n",
    "\n",
    "print(f\"Best neighbors for Euclidean distance: {best_neighbors_euclidean}, Train set accuracy:{best_accuracy_euclidean:.4f}, Test Set Accuracy: {test_accuracy_euclidean:.4f}\")\n",
    "print(f\"Best neighbors for Manhattan distance: {best_neighbors_manhattan},Train set accuracy:{best_accuracy_manhattan:.4f}, Test Set Accuracy: {test_accuracy_manhattan:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9328bdb4",
   "metadata": {},
   "source": [
    "Test accuracy is close but a little bit higher. Also, distnace metric isn't effective with this data. (I checked with other k values too.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "dce715b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best min_samples_leaf=20, Train set accuracy:0.7112 Test Set Accuracy: 0.7200\n"
     ]
    }
   ],
   "source": [
    "#GERMAN STATLOG  DECISION TREE\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "cat_columns = [0, 2, 3, 5, 6, 8, 9, 11, 13, 14, 16]  # categorical columns\n",
    "\n",
    "# Preprocessing pipeline to handle categorical columns\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', categorical_transformer, cat_columns)\n",
    "    ])\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# List of min_samples_leaf values to experiment with\n",
    "min_samples_leaf_values = [1, 5, 10, 20, 25, 30]  # You can adjust these values\n",
    "\n",
    "best_avg_accuracy = 0\n",
    "best_min_samples_leaf = 0\n",
    "\n",
    "\n",
    "for min_samples_leaf in min_samples_leaf_values:\n",
    "    # Combine preprocessing with the classifier\n",
    "    clf = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', DecisionTreeClassifier(min_samples_leaf=min_samples_leaf, random_state=42))\n",
    "    ])\n",
    "\n",
    "    # Perform cross-validation on the training data with 5 folds\n",
    "    scores = cross_val_score(clf, X_train, y_train, cv=5)\n",
    "\n",
    "    # Calculate average accuracy across folds\n",
    "    avg_accuracy = np.mean(scores)\n",
    "\n",
    "    if avg_accuracy > best_avg_accuracy:\n",
    "        best_avg_accuracy = avg_accuracy\n",
    "        best_min_samples_leaf = min_samples_leaf\n",
    "\n",
    "# Fit the best model on the entire training set\n",
    "best_model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(min_samples_leaf=best_min_samples_leaf,  random_state=42))\n",
    "])\n",
    "\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_accuracy = best_model.score(X_test, y_test)\n",
    "\n",
    "print(f\"Best min_samples_leaf={best_min_samples_leaf}, Train set accuracy:{best_avg_accuracy:.4f} Test Set Accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "c75cb5fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best max_features=0.4, Train set accuracy: 0.7400, Test Set Accuracy: 0.7450\n"
     ]
    }
   ],
   "source": [
    "#GERMAN STATLOG RF\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "\n",
    "# Assuming X contains features and y contains target variable\n",
    "\n",
    "# Assuming cat_columns contains the indices of categorical columns in your dataset\n",
    "cat_columns = [0, 2, 3, 5, 6, 8, 9, 11, 13, 14, 16]  # Replace with your actual categorical columns\n",
    "\n",
    "# Preprocessing pipeline to handle categorical columns\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', categorical_transformer, cat_columns)\n",
    "    ])\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# List of max_features values to experiment with\n",
    "max_features_values = [ 0.3, 0.4, 0.5, 0.6, 0.7]  # You can adjust or expand these values\n",
    "\n",
    "best_avg_accuracy = 0\n",
    "best_max_features = 0\n",
    "\n",
    "for max_feature in max_features_values:\n",
    "    # Combine preprocessing with the classifier\n",
    "    clf = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', RandomForestClassifier(n_estimators=500, min_samples_leaf=5, max_features=max_feature, random_state=42))\n",
    "    ])\n",
    "\n",
    "    # Perform cross-validation on the training data with 5 folds\n",
    "    scores = cross_val_score(clf, X_train, y_train, cv=5)\n",
    "\n",
    "    # Calculate average accuracy across folds\n",
    "    avg_accuracy = np.mean(scores)\n",
    "\n",
    "    if avg_accuracy > best_avg_accuracy:\n",
    "        best_avg_accuracy = avg_accuracy\n",
    "        best_max_features = max_feature\n",
    "\n",
    "# Fit the best model on the entire training set\n",
    "best_model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=500, min_samples_leaf=5, max_features=best_max_features, random_state=42))\n",
    "])\n",
    "\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_accuracy = best_model.score(X_test, y_test)\n",
    "\n",
    "print(f\"Best max_features={best_max_features}, Train set accuracy:{best_avg_accuracy: .4f}, Test Set Accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "2cdd856a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'classifier__learning_rate': 0.01, 'classifier__max_depth': 5, 'classifier__n_estimators': 200}\n",
      "Best Cross-Validation Score: 0.7350\n",
      "Test Set Score: 0.7300\n"
     ]
    }
   ],
   "source": [
    "##GERMAN STATLOG GBT\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "\n",
    "# Assuming X contains features and y contains target variable\n",
    "\n",
    "# Assuming cat_columns contains the indices of categorical columns in your dataset\n",
    "cat_columns = [0, 2, 3, 5, 6, 8, 9, 11, 13, 14, 16]  # Replace with your actual categorical columns\n",
    "\n",
    "# Preprocessing pipeline to handle categorical columns\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', categorical_transformer, cat_columns)\n",
    "    ])\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Combine preprocessing with the classifier\n",
    "clf = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', GradientBoostingClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# Parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'classifier__max_depth': [3, 5, 7],  # Adjust the depth values\n",
    "    'classifier__n_estimators': [50, 100, 200],  # Adjust the number of trees\n",
    "    'classifier__learning_rate': [0.1, 0.01, 0.001]  # Adjust the learning rate\n",
    "}\n",
    "\n",
    "# GridSearchCV with cross-validation on the training set\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=5)\n",
    "\n",
    "# Fit the model on the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "print(f\"Best Cross-Validation Score: {best_score:.4f}\")\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "test_score = grid_search.score(X_test, y_test)\n",
    "print(f\"Test Set Score: {test_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d869f369",
   "metadata": {},
   "source": [
    "In all methods, test and train accuracies, also model's accuracies are close. The best result is given by Random forest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a116dad9",
   "metadata": {},
   "source": [
    "### 2. dataset: IEEE New England 39-bus test case: Dataset for the Transient Stability Assessment\n",
    "Data link:https://zenodo.org/records/7350829\n",
    "Data size: 3120 x350\n",
    "The data is binary classification. All data is numeric. Data consist of speed, angle, voltage etc. values and it predicts if the system is stable or not. It also has class imbalance. (less than 20% of the data is unstable) \n",
    "\n",
    "**Data variables describtion from the website:**\n",
    "WmGx - rotor speed for each generator Gx, from G1 to G10,\n",
    "DThetaGx - rotor angle deviation for each generator Gx, from G1 to G10,\n",
    "ThetaGx - rotor mechanical angle for each generator Gx, from G1 to G10,\n",
    "VtGx - stator voltage for each generator Gx, from G1 to G10,\n",
    "IdGx - stator d-component current for each generator Gx, from G1 to G10,\n",
    "IqGx - stator q-component current for each generator Gx, from G1 to G10,\n",
    "LAfvGx - pre-fault power load angle for each generator Gx, from G1 to G10,\n",
    "LAlvGx - post-fault power load angle for each generator Gx, from G1 to G10,\n",
    "PfvGx - pre-falut value of the generator active power for each generator Gx, from G1 to G10,\n",
    "PlvGx - post-falut value of the generator active power for each generator Gx, from G1 to G10,\n",
    "QfvGx - pre-falut value of the generator reactive power for each generator Gx, from G1 to G10,\n",
    "QlvGx - post-falut value of the generator reactive power for each generator Gx, from G1 to G10,\n",
    "VAfvBx - pre-fault bus voltage magnitude in phase A for each bus Bx, from B1 to B39,\n",
    "VBfvBx - pre-fault bus voltage magnitude in phase B for each bus Bx, from B1 to B39,\n",
    "VCfvBx - pre-fault bus voltage magnitude in phase C for each bus Bx, from B1 to B39,\n",
    "VAlvBx - post-fault bus voltage magnitude in phase A for each bus Bx, from B1 to B39,\n",
    "VBlvBx - post-fault bus voltage magnitude in phase B for each bus Bx, from B1 to B39,\n",
    "VClvBx - post-fault bus voltage magnitude in phase C for each bus Bx, from B1 to B39,\n",
    "Stability - binary indicator (0/1) that determines if the power system was stable or unstable (0 - stable, 1 - unstable); this is the label variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6a81167a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        WmG1    WmG2    WmG3    WmG4    WmG5    WmG6    WmG7    WmG8    WmG9  \\\n",
      "3115  1.0021  1.0068  1.0076  1.0122  1.0140  1.0127  1.0128  1.0113  1.0475   \n",
      "3116  1.0021  1.0068  1.0076  1.0122  1.0139  1.0127  1.0127  1.0113  1.0475   \n",
      "3117  1.0030  1.0126  1.0255  1.0200  1.0207  1.0201  1.0207  1.0131  1.0157   \n",
      "3118  1.0013  1.0048  1.0041  1.0048  1.0060  1.0054  1.0050  1.0056  1.0056   \n",
      "3119  1.0028  1.0120  1.0259  1.0186  1.0193  1.0187  1.0191  1.0122  1.0146   \n",
      "\n",
      "       WmG10  ...  VAlvB37  VBlvB37  VClvB37  VAlvB38  VBlvB38  VClvB38  \\\n",
      "3115  1.0088  ...  0.66943  0.66943  0.66943  0.43879  0.43879  0.43879   \n",
      "3116  1.0088  ...  0.67039  0.67039  0.67039  0.43913  0.43913  0.43913   \n",
      "3117  1.0141  ...  0.91193  0.91193  0.91193  1.00220  1.00220  1.00220   \n",
      "3118  1.0066  ...  1.02900  1.02900  1.02900  1.07950  1.07950  1.07950   \n",
      "3119  1.0132  ...  0.93351  0.93351  0.93351  1.01600  1.01600  1.01600   \n",
      "\n",
      "      VAlvB39  VBlvB39  VClvB39  Stability  \n",
      "3115  1.00170  1.00170  1.00170          1  \n",
      "3116  1.00200  1.00200  1.00200          1  \n",
      "3117  0.98582  0.98582  0.98582          1  \n",
      "3118  1.09920  1.09920  1.09920          0  \n",
      "3119  0.99342  0.99342  0.99342          1  \n",
      "\n",
      "[5 rows x 355 columns]\n"
     ]
    }
   ],
   "source": [
    "##POWER GRID FEATURES\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Replace 'file_path' with the path to your CSV file\n",
    "file_path = r'C:\\Users\\DELL\\Downloads\\GridDictionary2.csv'\n",
    "# Read the CSV file into a DataFrame\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows of the DataFrame to verify it's been loaded properly\n",
    "print(data.tail())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c75e7dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With 1 neighbors, Euclidean Distance, Average Accuracy: 0.9928\n",
      "With 2 neighbors, Euclidean Distance, Average Accuracy: 0.9824\n",
      "With 3 neighbors, Euclidean Distance, Average Accuracy: 0.9836\n",
      "With 4 neighbors, Euclidean Distance, Average Accuracy: 0.9828\n",
      "With 5 neighbors, Euclidean Distance, Average Accuracy: 0.9852\n",
      "With 6 neighbors, Euclidean Distance, Average Accuracy: 0.9844\n",
      "With 7 neighbors, Euclidean Distance, Average Accuracy: 0.9808\n",
      "With 8 neighbors, Euclidean Distance, Average Accuracy: 0.9780\n",
      "With 9 neighbors, Euclidean Distance, Average Accuracy: 0.9772\n",
      "With 10 neighbors, Euclidean Distance, Average Accuracy: 0.9756\n",
      "With 11 neighbors, Euclidean Distance, Average Accuracy: 0.9784\n",
      "With 12 neighbors, Euclidean Distance, Average Accuracy: 0.9724\n",
      "With 13 neighbors, Euclidean Distance, Average Accuracy: 0.9724\n",
      "With 14 neighbors, Euclidean Distance, Average Accuracy: 0.9724\n",
      "With 15 neighbors, Euclidean Distance, Average Accuracy: 0.9756\n",
      "With 16 neighbors, Euclidean Distance, Average Accuracy: 0.9740\n",
      "With 17 neighbors, Euclidean Distance, Average Accuracy: 0.9744\n",
      "With 18 neighbors, Euclidean Distance, Average Accuracy: 0.9736\n",
      "With 19 neighbors, Euclidean Distance, Average Accuracy: 0.9740\n",
      "With 20 neighbors, Euclidean Distance, Average Accuracy: 0.9720\n",
      "With 1 neighbors, Manhattan Distance, Average Accuracy: 0.9916\n",
      "With 2 neighbors, Manhattan Distance, Average Accuracy: 0.9836\n",
      "With 3 neighbors, Manhattan Distance, Average Accuracy: 0.9844\n",
      "With 4 neighbors, Manhattan Distance, Average Accuracy: 0.9812\n",
      "With 5 neighbors, Manhattan Distance, Average Accuracy: 0.9828\n",
      "With 6 neighbors, Manhattan Distance, Average Accuracy: 0.9796\n",
      "With 7 neighbors, Manhattan Distance, Average Accuracy: 0.9804\n",
      "With 8 neighbors, Manhattan Distance, Average Accuracy: 0.9764\n",
      "With 9 neighbors, Manhattan Distance, Average Accuracy: 0.9788\n",
      "With 10 neighbors, Manhattan Distance, Average Accuracy: 0.9744\n",
      "With 11 neighbors, Manhattan Distance, Average Accuracy: 0.9768\n",
      "With 12 neighbors, Manhattan Distance, Average Accuracy: 0.9720\n",
      "With 13 neighbors, Manhattan Distance, Average Accuracy: 0.9760\n",
      "With 14 neighbors, Manhattan Distance, Average Accuracy: 0.9724\n",
      "With 15 neighbors, Manhattan Distance, Average Accuracy: 0.9740\n",
      "With 16 neighbors, Manhattan Distance, Average Accuracy: 0.9712\n",
      "With 17 neighbors, Manhattan Distance, Average Accuracy: 0.9716\n",
      "With 18 neighbors, Manhattan Distance, Average Accuracy: 0.9707\n",
      "With 19 neighbors, Manhattan Distance, Average Accuracy: 0.9728\n",
      "With 20 neighbors, Manhattan Distance, Average Accuracy: 0.9691\n",
      "Test Accuracy with best parameters (Distance: Euclidean, Neighbors: 1): 0.9936\n"
     ]
    }
   ],
   "source": [
    "##PowerGrid NN\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming your data is loaded into a DataFrame named 'data'\n",
    "# Extracting features (X) and target (y)\n",
    "X = data.drop('Stability', axis=1)  # Replace 'target_column_name' with your target column\n",
    "y = data['Stability']  # Replace 'target_column_name' with your target column\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# List of n_neighbors values to experiment with\n",
    "n_neighbors_values = list(range(1, 21))  # Adjust or expand as needed\n",
    "distances = ['euclidean', 'manhattan']\n",
    "\n",
    "best_accuracy = 0\n",
    "best_n_neighbors = None\n",
    "best_distance = None\n",
    "\n",
    "for distance in distances:\n",
    "    for n_neighbors in n_neighbors_values:\n",
    "        # Create K-Nearest Neighbors classifier\n",
    "        knn = KNeighborsClassifier(n_neighbors=n_neighbors, metric=distance)\n",
    "        \n",
    "        # Perform cross-validation on the training data\n",
    "        scores = cross_val_score(knn, X_train, y_train, cv=5)\n",
    "        \n",
    "        # Calculate average accuracy across folds\n",
    "        avg_accuracy = scores.mean()\n",
    "        \n",
    "        print(f\"With {n_neighbors} neighbors, {distance.capitalize()} Distance, Average Accuracy: {avg_accuracy:.4f}\")\n",
    "        \n",
    "        # Check if this model is the best so far\n",
    "        if avg_accuracy > best_accuracy:\n",
    "            best_accuracy = avg_accuracy\n",
    "            best_n_neighbors = n_neighbors\n",
    "            best_distance = distance\n",
    "\n",
    "# Now, train the model on the full training set with the best parameters found\n",
    "best_knn = KNeighborsClassifier(n_neighbors=best_n_neighbors, metric=best_distance)\n",
    "best_knn.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on the separate test set\n",
    "test_accuracy = best_knn.score(X_test, y_test)\n",
    "print(f\"Test Accuracy with best parameters (Distance: {best_distance.capitalize()}, Neighbors: {best_n_neighbors}): {test_accuracy:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ecdaba",
   "metadata": {},
   "source": [
    "Test is a little better. But overall, all trials gave a very good accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "054a9eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With min_samples_leaf=1, Average Accuracy: 0.9904\n",
      "With min_samples_leaf=5, Average Accuracy: 0.9864\n",
      "With min_samples_leaf=10, Average Accuracy: 0.9864\n",
      "With min_samples_leaf=20, Average Accuracy: 0.9816\n",
      "With min_samples_leaf=25, Average Accuracy: 0.9756\n",
      "With min_samples_leaf=30, Average Accuracy: 0.9768\n",
      "Test Accuracy with best min_samples_leaf=1: 0.9920\n"
     ]
    }
   ],
   "source": [
    "##Power Grid Decision tree\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming your data is loaded into a DataFrame named 'data'\n",
    "# Extracting features (X) and target (y)\n",
    "X = data.drop('Stability', axis=1)  # Replace 'target_column_name' with your target column\n",
    "y = data['Stability']  # Replace 'target_column_name' with your target column\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# List of min_samples_leaf values to experiment with\n",
    "min_samples_leaf_values = [1, 5, 10, 20, 25, 30]  # Adjust as needed\n",
    "\n",
    "best_accuracy = 0\n",
    "best_min_samples_leaf = None\n",
    "\n",
    "for min_samples_leaf in min_samples_leaf_values:\n",
    "    # Create Decision Tree Classifier\n",
    "    clf = DecisionTreeClassifier(min_samples_leaf=min_samples_leaf, random_state=42)\n",
    "    \n",
    "    # Perform cross-validation on the training data\n",
    "    scores = cross_val_score(clf, X_train, y_train, cv=5)\n",
    "    \n",
    "    # Calculate average accuracy across folds\n",
    "    avg_accuracy = scores.mean()\n",
    "    \n",
    "    print(f\"With min_samples_leaf={min_samples_leaf}, Average Accuracy: {avg_accuracy:.4f}\")\n",
    "    \n",
    "    # Check if this model is the best so far\n",
    "    if avg_accuracy > best_accuracy:\n",
    "        best_accuracy = avg_accuracy\n",
    "        best_min_samples_leaf = min_samples_leaf\n",
    "\n",
    "# Now, train the model on the full training set with the best parameter found\n",
    "best_clf = DecisionTreeClassifier(min_samples_leaf=best_min_samples_leaf, random_state=42)\n",
    "best_clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on the separate test set\n",
    "test_accuracy = best_clf.score(X_test, y_test)\n",
    "print(f\"Test Accuracy with best min_samples_leaf={best_min_samples_leaf}: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b06a0e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With max_features=0.3, Average Accuracy: 0.9932\n",
      "With max_features=0.4, Average Accuracy: 0.9928\n",
      "With max_features=0.5, Average Accuracy: 0.9936\n",
      "With max_features=0.6, Average Accuracy: 0.9936\n",
      "With max_features=0.7, Average Accuracy: 0.9936\n",
      "Test Accuracy with best max_features=0.5: 0.9904\n"
     ]
    }
   ],
   "source": [
    "##POWER GRID RF\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming your data is loaded into a DataFrame named 'data'\n",
    "# Extracting features (X) and target (y)\n",
    "X = data.drop('Stability', axis=1)  # Replace 'target_column_name' with your target column\n",
    "y = data['Stability']  # Replace 'target_column_name' with your target column\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# List of max_features values to experiment with\n",
    "max_features_values = [ 0.3, 0.4, 0.5, 0.6, 0.7]  # Adjust or expand as needed\n",
    "\n",
    "best_accuracy = 0\n",
    "best_max_features = None\n",
    "\n",
    "for max_feature in max_features_values:\n",
    "    # Create Random Forest Classifier\n",
    "    rf = RandomForestClassifier(n_estimators=500, min_samples_leaf=5, max_features=max_feature, random_state=42)\n",
    "    \n",
    "    # Perform cross-validation on the training data\n",
    "    scores = cross_val_score(rf, X_train, y_train, cv=5)\n",
    "    \n",
    "    # Calculate average accuracy across folds\n",
    "    avg_accuracy = scores.mean()\n",
    "    \n",
    "    print(f\"With max_features={max_feature}, Average Accuracy: {avg_accuracy:.4f}\")\n",
    "    \n",
    "    # Check if this model is the best so far\n",
    "    if avg_accuracy > best_accuracy:\n",
    "        best_accuracy = avg_accuracy\n",
    "        best_max_features = max_feature\n",
    "\n",
    "# Train the model on the full training set with the best max_features found\n",
    "best_rf = RandomForestClassifier(n_estimators=500, min_samples_leaf=5, max_features=best_max_features, random_state=42)\n",
    "best_rf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on the separate test set\n",
    "test_accuracy = best_rf.score(X_test, y_test)\n",
    "print(f\"Test Accuracy with best max_features={best_max_features}: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "44650aae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 200}\n",
      "Best Cross-validation Accuracy: 0.7858153733048849\n",
      "Test Set Accuracy: 0.7694915254237288\n"
     ]
    }
   ],
   "source": [
    "##POWER GRID GBT\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and test sets (adjust test_size as needed)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the parameter grid for tuning\n",
    "param_grid = {\n",
    "    'learning_rate': [0.1, 0.2, 0.3],  # Learning rate values to try\n",
    "    'n_estimators': [100, 200],  # Number of trees to try\n",
    "    'max_depth': [3, 5, 7]  # Depth of trees to try\n",
    "}\n",
    "\n",
    "#since the data is large, code runs for too long when i tried for 5 alternatives each\n",
    "\n",
    "# Create the Gradient Boosting Classifier\n",
    "gbt = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Use GridSearchCV to search for the best parameters\n",
    "grid_search = GridSearchCV(gbt, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)  # Using the training set for tuning\n",
    "\n",
    "# Get the best parameters and the best score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best Cross-validation Accuracy:\", best_score)\n",
    "\n",
    "# Get the best model based on the best parameters\n",
    "best_gbt = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "test_accuracy = best_gbt.score(X_test, y_test)\n",
    "print(\"Test Set Accuracy:\", test_accuracy)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e90fc04",
   "metadata": {},
   "source": [
    "Train and test results are very close in all of them but GBT. In GBT, the results are much lower than the other 3. Other than GBT, the others gave very high scores of accuracy. The best results are obtained by NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4d7bd359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Marital status  Application mode  Application order  Course  \\\n",
      "4419               1                 1                  6    9773   \n",
      "4420               1                 1                  2    9773   \n",
      "4421               1                 1                  1    9500   \n",
      "4422               1                 1                  1    9147   \n",
      "4423               1                10                  1    9773   \n",
      "\n",
      "      Daytime/evening attendance\\t  Previous qualification  \\\n",
      "4419                             1                       1   \n",
      "4420                             1                       1   \n",
      "4421                             1                       1   \n",
      "4422                             1                       1   \n",
      "4423                             1                       1   \n",
      "\n",
      "      Previous qualification (grade)  Nacionality  Mother's qualification  \\\n",
      "4419                           125.0            1                       1   \n",
      "4420                           120.0          105                       1   \n",
      "4421                           154.0            1                      37   \n",
      "4422                           180.0            1                      37   \n",
      "4423                           152.0           22                      38   \n",
      "\n",
      "      Father's qualification  ...  Curricular units 2nd sem (credited)  \\\n",
      "4419                       1  ...                                    0   \n",
      "4420                       1  ...                                    0   \n",
      "4421                      37  ...                                    0   \n",
      "4422                      37  ...                                    0   \n",
      "4423                      37  ...                                    0   \n",
      "\n",
      "      Curricular units 2nd sem (enrolled)  \\\n",
      "4419                                    6   \n",
      "4420                                    6   \n",
      "4421                                    8   \n",
      "4422                                    5   \n",
      "4423                                    6   \n",
      "\n",
      "      Curricular units 2nd sem (evaluations)  \\\n",
      "4419                                       8   \n",
      "4420                                       6   \n",
      "4421                                       9   \n",
      "4422                                       6   \n",
      "4423                                       6   \n",
      "\n",
      "      Curricular units 2nd sem (approved)  Curricular units 2nd sem (grade)  \\\n",
      "4419                                    5                         12.666667   \n",
      "4420                                    2                         11.000000   \n",
      "4421                                    1                         13.500000   \n",
      "4422                                    5                         12.000000   \n",
      "4423                                    6                         13.000000   \n",
      "\n",
      "      Curricular units 2nd sem (without evaluations)  Unemployment rate  \\\n",
      "4419                                               0               15.5   \n",
      "4420                                               0               11.1   \n",
      "4421                                               0               13.9   \n",
      "4422                                               0                9.4   \n",
      "4423                                               0               12.7   \n",
      "\n",
      "      Inflation rate   GDP    Status  \n",
      "4419             2.8 -4.06  Graduate  \n",
      "4420             0.6  2.02   Dropout  \n",
      "4421            -0.3  0.79   Dropout  \n",
      "4422            -0.8 -3.12  Graduate  \n",
      "4423             3.7 -1.70  Graduate  \n",
      "\n",
      "[5 rows x 37 columns]\n"
     ]
    }
   ],
   "source": [
    "##Student dropout\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Replace 'file_path' with the path to your CSV file\n",
    "file_path = r'C:\\Users\\DELL\\Downloads\\studentdropout.xlsx'\n",
    "# Read the CSV file into a DataFrame\n",
    "data = pd.read_excel(file_path)\n",
    "\n",
    "# Display the first few rows of the DataFrame to verify it's been loaded properly\n",
    "print(data.tail())"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGEAAAQyCAYAAAC4U5HpAAAgAElEQVR4Ae1dC5bjIA7sc+VAOU9Ok8Nsds/ifQIKhJAwdvyhezTv9cSx+YgqCXAo45///Pd/i//di8EPEeD/7kXASbgX/1C7k+AkTIDABCZ4JDgJEyAwgQkeCU7CBAjAhPdz+fn5WR6vD85c9qlHwue1PH5+glE/j+fyXjXnvTwfKf3PSPrVAtUEn9fjPKCmJmHAOwAOedLP5SS8lyfV+3gtX/nwoSRss6kfCc/nQANThc/X8grRcF4kqOGxbGuwXsayLPOS8F7g5U+rT0rGP98fJyEzvM0xViLhvSwYH1QWOPD8OFuzLMtn+byfyyOPGdRlPZanNgBmQskxY/9PaV9wAOGt7yfGIfGZbd1e9/rA/Fner2cZM6m7frzyuLnLJvUHvAp4C1zCNw7g0XA9HSLp8Xgur/d7eb9feRBvGpxAjmPLY3lQd/iwSVg+sbwwicjlv5d3Ghz21N3YxP0p9FqR8MfztbypPUQI2YgBaYdN65FARggPhF2RdRigk0BAfWAgMlr9eCYBZeYM8UC1oxP6O+ruk5DauDoJ2GbTGAkqaKkiFvrjA7NBmAoyI0K93mkwy1oOd9YdCkBeis7Gs0oVKl7ssjgcJIGCIYYhMEeo4zv1/TYJ1De/lhd1L+kv3oeImZQKMrNYvb5GwkF1w4xP6U5/HjS2vZWp8TabhklA//8TUE+AV2FpkECDMm78ms+TSTiybpCQPsmpyg2q7D47JGg2rQ/MqJ2BrHoku44sOToeC01hyz8trT325HxqvVaDUcdBdWcj6gOa+YWJROkSqCHG/ZVu03gkhMkQpo3UNQkvzoDz85Yx+JmDp91LAhomyjKB2Fl3jT37ltpYkbDNpk0kZIapW6kqJZv0ijGWYEr3qubYAjjV03l79R/ZUAf9zvV80tQ23ljg/CF1hybSlJzKT9NT1iVJOFD3iE0bSWgH6AKRTkIgJ994UQTF7iEaeQwJwTnYzWCZZn6W11F1h4Z+wk1kNcaF+xPe1QIRRFy6ryg3Eo1NOgkoxz8vQcBJuATmfiVOQh+fS646CZfA3K/ESejjc8lVJ+ESmPuVBBJcEOyC4L6b/ANXvTuagGQnwUmYAIEJTPBIcBImQGACEzwSnIQNCKytNWwoarakA5GAdQJtIefC5igkQGxQ1g8utIetNH5b/zoJWEMNi/RiEebKNt9GgrVES9q3Y1TiqyTEFbBHWDakBW25jHcZDwoJ19Rtk3BU/SsksEXsShp5VPUbyvlnSUgNj96PscHokkLaqL+RAmBaeG9WYXelF0/SmMS0a8u1DWNC4bxYL/VS6A469UvRcBCKVbKf5IBUhqo7ytdraUu3D0ygkjiWui0SAJPaLi+Kw3A4/670IySk6CXgkg0QaWEAze3IIuIi5EKaYOaKuFfX6MJZqX7qxpMCJBEJJQhgCGXYJLCuqOQwRE1FMxSk7JXbAxShUktetDV9BVIqg5/LAGd1QzI+AFqOh0XKpn6ptFmrv5UEdXCwSEBjWgeOshXZRt0rYqNRFjf2kPQNCfBCo8vMzqQdWHkTeJXkM+XfWL+Jg05CMSg+U0DPAaS/pOOpACWbGoNYQ7Vr2jlk0a4NnesAhrLz56BQeFMkrNSf2lBFCZ1TScBMSA5I/Lv0DA0kNFi7pp3bmr4pYwUEVn4eq3ibwrGMok6ZW+tP6YdIiGEj+nA0IDh9VJVVXVVjUMmAMDw8fVNnAqzRyRZbilxzUCi8KRJKD4InvKqatZs7akMbCf2CQqEWo8GTJHkARpxHGfRM2oaBvOoGGxKKTLNKR0Z/3unBDsuzIVuUkdDBY2P94VFf2V4qoyEB4FRuy/mkYwDLDEa+FN5xivrIU9QGlJ3pq3JSGdU53pWKKSq6Acz/IeztipRZ5Etxrz4OApt6ihqfwxNT7AClQgIM7HLADMvpGCCfSnktwz4R+kX67BKsjHyODj7k1VzGH5XaJeDamzl6fiK2nTlWLhRRIsS9Vv2kUCd1OBtv4oOTxYJStEJCvrj1wDTIKGhreqOYv3C67Y72tmorqFvT77XrF+RzEiYgyUn4UyRM0JjfasJxkfBbEZjA7kAC3Sv4330YeCTMEgkT2PFPm+CRMAH9ToKTMAECE5jgkeAkTIDABCb8nkj4wz/4DZCAlSUXBMugwbJttagkEw18XycheWBcGdIWPAZqOSKJEglHgdA3z1oOdUFwH7dDr9okHFXNSiQkA2gNE2u3eT3zKBMGy1EiYTDnl8nuJiE1POKOscHokkLaqJxwQXC7i7ALgtOuYBhA81jiguC0i6LU4WQ5jdAjad2Rci4DXIuZggLjDUXWlp2DYY9UHFIn16kf8prSF6ZuTbaXymh0RykXGiOHgCgLEQAZBsEAlAVPDOeVBmxO35Sx0mWiAvXTytsZEzbWb+Kgk1AMckHwFhI6aYn4RFoVJXROJQEzISZegoIsf8rwbLyCuZt2TTuHLNq1oXMrILDyuTArtym0V048OmU2NnXSbiUhho3S5aRGqCq9xiC0uNzUVF3bEembMhIILgiW5AEYcT4BePSTOnCQavwhf3BBsCGEZd3dqIC4AreJhPCEd9GAuiA4imexI3DpnHKfll/XtVVAnMvSSEhe74LgjFLnwAKwk+WvXtJnR3tauxXUren32PRL8jgJExDlJPwpEiZozG814bhI+K0ITGB3IMHFwPeJgQl7j4SbI8FJuJkAqt5JcBImQGACEzwSnIQJEJjABI8EJ2ECBCYwYSAS9F0Tw+4o7SZyEzTp95mwQgIUF3HPu/Cq9vS69tvFwb8Pa9PiLglYqyV5RrtBDMn8tPNmXX7BQMAmIS24/EhZi1GQn96PgEkCoqCSqKzWs1EEq+67jS5QaH+SU5A97/xW2cfygqxxaet+PF5Lvpxtb98iSwIDbePenOXkA4MEACEkKl1jkKfecgwCK3VX3B0kxLGIdvF6hp29QAKcBtuphS2SH9J+2Mjep5x3KJNpu4099KJBAnRCwhs7VUNjWcn7QnqUJRrJPLsuFkCJutE9SjFtyJzyrHSdsLGJbqgNmwu1ZWd9O4gEA7hkNRqv6YXadhtlJRKqMjIqyEPdUzuFiMlAFHU9n+VT/aX97VZIzNUdfGCQwBpltakyJHm71Qh4MUd8ZyToJETBFzYlDw9jNDM3RCS0UMqnZX/V1uO/GCSUvUU5bnb1E5CQjPu8y+7vtbyy2NhGQooMu4GnXjFJyBLuIe9A5Ih+HMBYu+LS9v7yQQ7ayjI8VSPK6nZHLUb0uFYYxLMXGeW2WS8/Y5OQwSCgtJsymhKWKWCenUhQaX/SoDfVB2Y5kGP8+JGq6o0k5M1zMwlFGd4ST90ZRcPl+IcKOyQIYe1PmhZWP1twYAF2PUWF7r9tODyT0qeXXYQIeCyPrZEQZjds2sm6JMYBPaGRHOJnoalsfPjlFTaRJTtbG68hpU9CsKG9uaG+Vv8Bb8OuuFQ2SdXZLr64aYpRtaU7UmwMDwRqrr0l7TQkXGPIv1zLQCT8y/Bc03Yn4Rqcu7U4CV14rrnoJFyDc7cWJ6ELzzUXMwl04H/3YeCC4GscvluLk9CF55qLTsI1OHdrcRK68Fxz0Um4BuduLU5CF55rLjoJJ+GMtZH653S9MidBx+XLs1gvET/JG6XqJEAGwnZhCQs0D6wlGKVdfZotY2qrB1ebk+sDfiNhsCzG05soJL/eXLzq/dKVqLJI3wDNSMgATHAQuyK+8tg3qh8JCpOkaICyTrncr23X1Q4Ju8o7O1PqioZEEtGWzSSEbPBAuSB/Svt+GQmbRQlr3ZHp6hh4fpY6if5QyVOoMKDOqPMmBlkjkA6CAfOTF5Ty06mwU3Ee19jbcD+1PknaV3ypXZPGWnhJUx9Fm62uSC9vXyQEMUZ81S5XKWBaRoZCzQBlHE/X23e7asTnvbzR/eUy6Vz6g6BXIYGUeEFFQQqRrOQmpUV6RTBUHomkyr6AKxyNKTlQn6qJpUy9qO2URz9hN/8wMPPGyUTMY/Ol4Z13YZCcwmmN0M6lGjU7k121Aq/ojgIxPDJRhujD4VANBEjfXCj7nraElvqbbJ/XyuyoyZHhXmCkVmFJRUc64Gp+jdied2mAqGUwHVXTJs2+dC48uzAuII5RLB2LYWCU93V31LRpoXGBRFVxWvt8PtNsShrXenjVFWUm23T50hYSQGZjsEZCqjOPJyMC4pSnKZ+sXSlvX3cEw8UA9AbgitHKTKoG3WpEOi+6i0DE2STQ0z6VjJ5FRfaEeIDIVjkACUZ5+yIBja+AKcTQ++7LP5yXkSD60NSNtI24g4SOzaVh7Ggtff/6dhJAQPO4kwVWeghDiYQcpmGmQtGjEGWMKQEB2MKZs8YEeCNPGwtRleDwbHXMkwJizQ5GER32yuuTYP5s8Vi0uXXsXqLglqaRrzylswAuxoV7gAag2BKUG8XDNGVMjwRqjT+IhOwg9BPNioAYAMMsgX/6mpxUK687JsiBiUghqTzvbaoaoyi43FTFG6QIoublbNZizr2pAkRTHGuyd55KAtXb3lyRI9SPZaWupuqaK1DYF708PRJYtvMPrW7s/JoPqSE5QnaMHYXeT4LZfexozQ1Z6hnePgNuJgGzBjHV3deWG3IdE8X3koA+fag/vQHji6q8l4SLGjl7NU7CBAwFElwMfJ8YmLD3SJglEiaw4582wSNhAvqdBCdhAgQmMMEjwUmYAIEJTPBImJYE/KYj1hOinshcTLisOVhE+ebn48uMHahIjwSQwFfW+I4sXLczUMnRSf4tEuR6HchR14KPhvrfKa8fCZKEIJCIS4zKpX8HtYNbupkEdAWZBCZV0XfvJYvbtVUprMVifi6XN1SuvsnvOW27U3DYHbKS4KTEzO6cPRxgoUmuibdl67sQ16XFIt95h7Gw/p52Jcba+2YSGrBSY+LiPikh6t17iwxyRViLrk5hoVlCVEkAePUWcHiWIis0gNFGEtDu/i7EKJx/FpWFtdXcNhIy4MxL8jl9ibKJHNjXgA4QWdkhrbKEqJCAeuTmhkW6IuzbREKybccKIOySMzmcJ61Vn4RqdhTHgviSayApVHTsdDyE8WM788KwKhgUwLFlaGmYRWCyorMtaFVXSK6VhXO9XYibxoduWN1iVNTTJ6G6T0jdibxN0EDK9rBQrMoCodR1vMr7GZrowEa5uhcXEpRoyTYUR6miZFMkRH0UnrXQdyHmFeK4ZxeIXYuE1k1QevkcIcEQwoY9q0tJzHMAemqEtKOps9fYg0hIdtq7EFcNSV96dl1GQqmofZeBZnQBLHi55a0NCf160M2VyBH1VKb0y0LSdhdiXOGfvbLKtX53JD2Ql4/jBhBciJ8qAEgihbXhfPGe15O6LTlQ6wDm2Yu8mz9ql2LYnD+NKM3X44FuV5yyxxnl6d0RGZKM1YSwxvPQMDwYqTmCSnypJ/TZ8lk1SU5Wew/sUhzGKjbFNnchFgzQV4xz1NbqOblHfnjm9EiIZrU3a62wljUggSyfO8spVBJCi8NNEe4NiMTuj47DuxRvtD8bmg4oGsP203FCgh2WMXPSSZCF+PcTEFgbE06o0ouUCDgJEpEbvjsJN4Auq3QSJCK3fveB+Vb4Y+WBBBcEuyB4Al+81wTvju7FP9TuJDgJEyAwgQkeCU7CBAhMYIJHgpMwAQITmKBHAluIiKs/6XfwZnONe1rQXa27x6Svau2TUEle0i6KxmrYV1ZszPxvkSCXFnOEKOu+G4H05AWBfiRIEsIae+yalEulVD/ahMBmEtAVZBLSei99d0GwgT2tZWNTXBLBuSA4AcWcp4auLLZwrRQUIC4IBlqK2gIRWkkdQ3pIYaDq20NCIoZLNmHLyifsqoRnQQmDic6a7qiaHcWxwAXBUozbY0GPqpijXOuPCZWI1wXBWTtEzkkbMvbwD9dSFKoRNEpCHn07tSldQ0ldjBjbaReGoetI+aUdTZ2lHhWYlL7qqtI5WXR5qEWfhrsgGOw2JIA8A7jO8wmyr14jASa4ILghAc8x/CwNqC4IJr9JXQX93LGy0y68DNNBFwTToNx2mMCpfCpeWS7iaKOgNpXpgmDg558nIVDGMX2KelK1XixHwEngaNx07CTcBDyv1kngaNx+7GPC7RSkNw66INgFwRP44r0meHd0L/6hdifBSZgAgQlM8EhwEiZAYAITPBKchAkQ+MaEoZ/x1yvQIyHLHWkhhqtvWIHJAOsyS7n70JKL7C7w6IwKCXtsXiVhbWHFSaiXVHUS+kKELgm0vWbYtkaTbFwQCUc77uHlKZGg1/EFCbS8ifXexuOdBFIXLGFPpWZDK0nFlyTknatkNKgk0Kvhn8uDbbAUFHumke1ba2kMytohs5Htbr0jOwEH27KgTX+dMUlehtug2SfOwYn5wzYQMORr9Atq8w8Dc3J/JK7kJAoJuT8MT/S8lzfbpqzKGyosKgzsnAuFW04rGhTtxGLItp2AiaTgtWELNGx9VvfnVP6mNmj2yXOfiEPs1uk1wvT9vYSdo4HzCAlFtsLEVQoJy0d7V7MeirmxMkqC0cktZIMYSK0SBKRCvVeX0UwwAICM8A1tULsjxeaMn6wL+/CNkcA8BKBpJKR21x/wXEYgKtd2euSZmwZpZZUMKrFNGUjfLwup4qeRVitbOwftVUNCwnWUhMwm3iCeKmsG7NCnvpZX2I3xuTwxw6oA16Ojbri29eZKvmRTFSUqKFSTAWwwgsaFtTZo9hnnOiQQrt0patUYMo43UiOBBuU88EFKj08eCStggo0GwJV83D6zDFwwSBhuQ8Ejj2EMo+pcl4S0vAmz8if6y8bNYfhjeb3i9KwkKdee1fsKcF4hoYqOXHs5aEjQyirJv++OUP5jWW/DbSSUijHlKiRYXoo9QTkJfQHvCyQ2JPTzPUMU6gNz7ZlEHADndm1rA3qGqmzFZr2u4jzbuqOUD1NWIqKQwAB6vsI0jKKldE+8sWGaU66lnXMxRc3doNqgBBQBnp8kwqNH7ZRTBSq0QyNhYxs0+7RzwXdTt4wpMgNuFwn5Bk6QEBjnTyn+xLCOpAkSgjNSlBQAaRo5erNGT0MWgjs7ARug2N7Z3kBS16S2QStbOxdIR48QyeDRo5OQPN4/rkHASbgG524tTkIXnmsuOgnX4NytxUnownPNRSfhGpy7tQQS6Pcj/7sPA4+Ero9ec9FJuAbnbi1OQheeay46Cdfg3K3FSejCc81FJ+EanLu1OAldeK65+LtIMH8mPh8srNqxZYDDKtVJwPJmWDhR1gGa6vlv5SPpmwLGTvyzJAzsCAwviUue35JgLTGWpVW+IDLG3gGpkgNcHwkkWwnRwF5c3bQngUZ7GQX5418mQaxfN1jsO9HvjkgXmrYuMz0ge4i+ZrvdrJkj4SYS8nqyygIHnh9L6McEvFxAADVH+ETdbEyoRbuWuJfsaDe8ojfTQtCRLc3ORIv9WPd+LC88IxOuSxLadj3oLey50M4B3zV4XYHXATcN4LGPttLhfK2OwCJ99SRQTzxL7UlAgaD4fuOy4N+OFaibvUs5K0AEoFXZlP4ZRAiZBAVPOM23uwavdkeh7mSgbGQ0Ao1Bg+sxAd1ZlrHkxkC6gvy4sN4djYp7UTcCCTWo0Z1JkPbkXOIgtVfRl4qEzVfYBTzHSFBlfBiQEXwaCdq5YpM0Jl5ZJwHGs5KUSQFAoq7ns3yqvzSl5gAajlbqkEdoG3VZ+YkKmUj5jnzFWQdJKKIoeBUAxHddx9MBlMyD95VC6KQ9IzOBahuWywmqPOhhxedXJEQBWxasfbFr8DAJdQjDy/jUtQMEbyz3jStIoIGyigIWFYotbZTxRPrxt7sGj5PAtZuqR2okaOdKQxBNdcOPioR+3cWKdKS2qUnVPUEztmo2p6Zu7dpAQnlQJM5OSp8W62oLp/N5BoGHS2CYtXMvJxtp8WkCpdetk5wKC9GBgkvXWDsEuz50mByo6l7bjBKTTSRU/WxTkQ5ElWdEwMuIi8+yRX1qaMpGEnjdq7sTm2W3IIYzYXrOpr7s+bwGGlkE+22OptkbSSie3VZkkUAWRJEt7g0okqhye1bBfxBkSmsTqH7ddPPF6yZym7rNsiWC+N7eBKrlIrn8pJ4AT7mqN2syg38/FQE9Ek6t0guXCDgJEpEbvjsJN4Auq3QSJCI3fHcSbgBdVhlIcDHwfWJgwt4jQbrlDd+dhBtAl1U6CRKRG747CTeALqt0EiQiN3x3Em4AXVbpJEhEbvg+Bwmbf0Y+Diks/LQ/zR9Xx1pJOgls0QEan/I5KglZq5pddxI6u0HmlbC4jRptpfakbXQYfuOHe9aOx0vfnTI5wLyRcKhlM5NwQnRv8Ip+d+QkbIByf9IvSRjbTRfqgjKuJBEWSGZjwp8S+uIhmw/W5oVCJbX7KxIws4iL9p0dgf9RoW/YjjQs5j/S1tV7SNAGZi5T3rKb7oC88a8JfdFBwVl/5O6XQ5Gg6TgtSSNqNMVbewZmTcqSzoVnDJikMYi55hL6ZkgsTIZIQJ9dSlOOBnfTHYiEVv2mkZDI1BwE57ijsPFGMV4/9XkVTdAXQt9SuNaOovr7akwgVXUlqgII4VP0f0eT8AuEvheQAHYHd9M9jATUK0kuTa6O9kRCVcAS3qcQZnbdnqFnl3Ht++7I6uMh75MgGYZQg02g9DwY6NruixSXNE4wFM2yWZrVw9TWLgmYhjLZZig3yiXj9Fxg8j0JrNKRHYH/QaFv2TEfWlhBAgZsVYuKH/BWmIfQt9yErewITF0SRLD8IXXTW/VIiI47KMg1y7bcf7BcM7toIznop9OOj7V1v1WBn9+JQIeExdq6f2dVns1CwEmwkLnwvJNwIdhWVU6Chcw05/U75mnM+zcMCSS4INgFwf+Gu3da6d1RB5yrLjkJVyHdqcdJ6IBz1SUn4SqkO/U4CR1wrrrkJFyFdKee30XC5p+lOy3feAkLSau/7m8sl5LrJGA9gdaKSdWwWjD/DX0k/WqBeoJ/lgS++KJDk/dOVZfwjDz2aWvZtLcMapd22JXkANdHgu8QXDgMJJwjHO53R75D8Bwk1BsQFpviEf+dnB+36d55U9gkBiZRldjK8p8SDtPOxcBkfaG/A24awKP0xEqH875DcHHNGpPV7ihkNGYl0XPRT6LgenaEqZ3vEMwoSJvAA5MxElT1nBREaSRo51pjahHXntmRVk869wuEw4MkFKEXpmjwcHz3HYJp5/r3wsV/xd34UetkwyTUAzS8zHcIJnh9h+DgZJ3uSD6YwZ2SHxvjHk+yduw7BCuvkkGXWY85CcqJhMPj3VGwPfVn9JtSGQzQKmX7fLrE8uTHr/CmDqlgjkXl+4UHPTf9B3cIFphsJKEdoBMDcRdgxRvjdd8huOCEI4aJerOGdP55CQJ6JFxStVcCBJwEIHHjp5NwI/io2kkAEjd+Ogk3go+qAwkuCHZBMBzin/307mgC6p0EJ2ECBCYwwSPBSZgAgQlM8EhwEhICB6xq7cUSCz/N8sjeAnfk0yOBC4KrjaRIuAWJy47arCxOgu8QTELmeSPhUMtaqUcOjBsjIW54dUJ058atH/S7IydhHcEDUnxJgu8Q3HLAhL4YTxXxc8n3WXlSZyUSMLPwHYIBKfRPA+LnkCV10epCP2ZHWaLCtu73HYKBePMJp2zlQJD91GNPTt8lAeHEP/nGr40ZdALeUKuzs/5Iy28OzFpZ6dwvEPpyeAB4EaKVtn05JkTQSYv5Ci+4iBETN6w9iwR4VXrYhDsIjjnRJsEcInF8+A7B5Xm7EiVltvgdCb5DMGOvgKoqs5MzHEwCwkk+9oTzZ0WCVT7Dgx/uiQSeP6iun0t4MrU7Uenb1XZHiOjnN7Mji3k80yxJ6BhpAqXnaRvEUJtI6Jut+gDwemDOmtvuwNxlvuhSH75DcMIbYNdTVGzeWwbllByz0G9I8B2Cs6+zAyb0TROFeB+ljhT0hInRHbEi/fB8BPTZ0fn1eg0MASeBgXHXoZNwF/KsXieBgXHXoZNwF/Ks3kACTVP97z4MPBKYR9516CTchTyr10lgYNx16CTchTyr10lgYNx16CTchTyr10lgYNx1eAEJ+sLMXQ2esd4BEgCitrPLSJOQX660jeRdT9NdZVvP3k1xZtm84nUSsEAdFij2AOkkcMC141USsOMj7Tu0T718Lglao37buRUS0poprTVjPXRl3bkFwEloManP9ElIXVHEfQDMz3t5pYgJi9tJJffSNqNiZdPeceUN50xCw0VYP7TborJOqyo1WlHug95kXrV9II1aNhXS5v2xRL+9dqb2dEmIXVEZB/oDFVcaJCUee+3vj9wQMBlHxhNhj6DgY9uygcywBVshqVEsKEBBSgIVSNgO+aHLTXpp9Jd1wxlrRQWc6CF7CtlObCkHEcDLUmUHj2FdUfagdI7LDNM1iyCcN0kQj1+V9GJ/PHSHsu6GhASSTJfbQAcjaYp0kROf7ZNg533tarJBZPOYGWuPGQmoTNaFgbruGeAdJWpKm41rDXiZzdg1yYoBnBFRBSjU91heYkP01qZeGo0ElK21k4bNFNUcHKudrD0GCaWy1/u9vPlf6iZKo6lpdoRkr1sFDxBpEUjXik1V3641ko8l1FdrO/eOpGnK7rWzkFb0puVcjVfdHp0EhApUztpnFe494zaAF3g4gITE58jOvd00d5IQw0r0balh9IGBr/QYBtAhj3GtaSAqOI4ElDiyc6+aprHRaEuq6MDuqF9RqC8Zx8MOxNRh91nemOVc2R0B/fxpEZsTlC61eBZ5W5y5sT5eb2eQbi/P0GMI51XKiLUWnNvuSAGYmxqPU6M4sKwLI+3l84lp5WN5dO4TatKodAuwYnR3TAh20K7CrzSWvZZnmipnfEfSBFNaErJ9BHh+nIxNrRlhAas9JIDpbHBEvflfTUcScHZvEID4oPsSMwrTuC9JoBsp2tqZj2OPp5gpjaRZGVRFHabo12xncao2ErN37HMAACAASURBVBq4/cTZCDgJZyM8UL6TMADS2UmchLMRHijfSRgA6ewkTsLZCA+UH0hwMfB9YmDC3iNhwFPPTOIknInuYNlOwiBQZyZzEs5Ed7BsJ2EQqDOTOQlnojtYtpMwCNSZyX4vCebPw2fCtafs8nN1tQbCirJJYAs02KEkirnSYomiwaJy1eU9VuFhh/8UCXnliK+UxW3P2hUxJ6F1siMiQVleI3UCVq2Uy60dZ5z5pyLBQjmB0KjqzgBcK9NJIFQQZmKjbxWcVjzbiHNDvqhSCOLgap1aeeW6UY/MS9JDVUQciI2bQ5UxL75yuB7u2rVoWktWhX1bxNDMsdYHZisSrEFYAQeCgKx+eD2XhxDnYtNwOh/EwZVaQ3lCSKknTwrCoj6pBovKoh2/oBYhxUQc7yBQKGnhaEy5QbZrshYoRIICY0AMfRQJmianPZcaUqn1mAU4TKA2otncuAEtz+e9fGo3LvIZUX8mTMpTPkReNAppGj/EzJFdQNpCYF1Gr9u+IBKKN9ni3J60xJhxKZEAPutP1M/lNtq5Olfubod2Ie6V17sW6zyEBOYQqmKNnvBBqAexlCbO7YGqXdPOhTbRLvZruxWnrkhER00D6664fokf5/y98k4lAYUPdBOpdduEtwwSDXDjHKbOZbDFVs48Enqgod6S5h32Wf0sH/mJpOgyMyn5ApvA8Pr59SVscaSvrCn9XpUV12XFGjhVRtqBUtltt5MP/W0/4opTPKupC85zEODl/JwwMs/+emmQR6tj5FpMs687AgHae2g6YMKsrOXkqIIY8dROTivPN/UUz63HZsgyazDzjE0ZmDF2gXw52IZ2iF2I9fI6YugCxkAkmD9bGPNvCU4gjE3x2LSRc4BZFbqRKCguetIGCFlPGNvTzykjuxUzR5JT1KI0R8TQ83SvJT4sQ+ON8jQOK29IDL2JBD4QhTlweuqldrdSZANOe7NDjYa35Yws3yfPxQlU9iRnTmzNptqbL+qaopfWkRCKCuLloqamuuihv7ppg/ZTgVvE0KwtdnfEEl1yyEi4pL6JKnESJiDDSXASGALeHSmvfWT4+OG5CMzTHZ3bzqlLzyTQgf/dh4H+s8XUvvP3jHMSJuDUSXASJkBgAhM8EpyECRCYwASPhL9NQm/FaYKWT2SCHglskQILLfyzWWRRG/T7SOiupqltPOZknwSxskbb59Affzm5bcasJFhLoYa8xm7gYVf6JFRrkFvr/H0kbG3hUemdhKOQ/KKcY0jYIoZNawdtkPUip10/rteDSfRFGlfojNL6tFBTQBXBx7dwDGPMdY1W1PzNjsA1X9bmtBiYYVydS3wrqgQoF2owxCL7ZhLa8qHowwQhD6hrYuCgNU3PV+S076KyVkmAc9TbrUFotmdHYA5gsJ1+wm7+gQRtYK7EVfZgBmAaMexGElAOAM+2MvEuKR1GxcBZyySFa1SwQgLqL1IYWADn0FWIjbg5Y/qqFR3m+5iRQUpe6HsVHfAS4e3BTuPaJhKMMoBD99PKa8+OWhKsMmLFIKhyEIXIlHpRN25fFmODEZBQAa61uNMgS0q4iYRe+dKeETEw5emU2QDYSRuK2iLptAn9cmDuGWlUegYJ1db/fHCmYxmlHZvvIIHq7I4Jq5FgAB2c1LjWNBQeraVPgDVAIg99Ip9U6+H8NyRYZcT6v++OUvnfkYA9T8U2+2lv0jgVFCAkEuqxpQzw0nMxraz63YD9O8kpLc/WxcCFNGEXlak4SK/+vTsClwfLk+1dErTZUfjZgik2MX7QiyiqZ82MnYGz55bnxeKU1kjPyscUGFNUEJmBGhEDB6xTlxXsjRrU4NsKCXkMgRZXvmxD3ItoRKa4UQfmYHuXBG12RGDLireKYenmLj0kSNGCJyIjmIqHrop325u5rhiYBmd2Y5fbo5IQwi6osXFvAJsbYbMRTT0SKDL1MSHm8v8vQsBJuAjoXjVOQg+di645CRcB3avGSeihc9E1J+EioHvVBBJomup/92HgkdBz0QuukfM7CRcA3avCSeihc9E1J+EioHvVOAk9dC665iRcBHSvGiehh85F1+4lwfzZ+KLWT1KNTQJfSOFrCo+0Ywtb09ndFichQLdOglhZqxY15MLOVjY2k2AtY26teK706yQoC/2H7RDsJAxGgkJCyJkAlIvym3zMSfiSBLZY3/LUbtSENeSKJEZCLeZtN3/CQn5UbzBdUVX5PtFueV/0Y/C5i6oVX3/Z1R2hVlVzk8lJA/j7vbzzTl6WZjOCWis1hNxyTcib690o2g2TDrL1uTxp1+KiRUEzT//8igRN2gFiKgelZmC2xS/kLk2QA5mi3HwQ5xUhL+qFBKYgB/GYqMOsu+S86ugrEtDwLBeBNw7tqqsLrdDwtmy6Ys2OjlTJwYLrPg8hoTg3vI712fweg465F7MxoWmyes0iwTqfSoXXF0NVpV1jw0UnviAB3sfDvICxvqtuPxK0rs6OhFKveg/5Z0lAH889G91RV7zL3Ev19ngd3RF3XpsEOISi3AvDkbKPaaduZuElh/siAQQoOwQDvDJOsHaIXXXh7c0TLej7m4HZBhtT2KZekk+mGVB1g/+rSDB/tjB2CM4ADuyqG4CACFiKiRW9a0/Iy+oND/TtFu0yp7nocD0SmoF1ZYfgYHh7s2bvChzHlGZX4MptORqGkDfVS1soV79vabsRU9pfEQm83X58KgJ2JJxarRfOEXASOBo3HTsJNwHPq3USOBo3HTsJNwHPq80k0IH/3YeBa1G5W9507CTcBDyv1kngaNx07CTcBDyv1kngaNx07CTcBDyv1kngaNx07CRsAj7+TJ/1T0+xldqmskpiQQJWrzqL9WmNoVnFKmV2j7qrb0rOremVIg47BVtIsEBvPn+dQ8ISxFrYCTh8pt1QIM7Ctb1CKTSkJtFerNfTH4brhoLgoPpa9oaCmqQiEprrC0CoF93bdN+dsUn4rtwjcycSKnHDMeU7CcM4zkxCWq9tI8UIX7G+C6VEHuywro0CRfqC2T4BcBAfow56y625nl1qsmyEiTFlu7auCqFDYpGWfj3t/Vvtjr4kgTaWpUEuLNBPsmtvg0eyMewWxmwsG+LC4QaE0FmfVdJ+3x19S0JocWdMUCIBjvG1ABgaqqF+3u6OYE8dGboQWkv7C0mA1+mzFDSymn0pREZv75dVR4RFAs4/l1b+mSQ6mWQ97S8koRM1hFoCvIqSU0lI9uRxRrnHyiQYaW8fEzZ3R5OSQDdwQer5WT7yM4dUsZ2nPSwSqvAPlRqhrnplMa5RVjfpjXJTQ2/rjoaE0Lrth5FQhT9TQzcPFzagEnq6cQFXJT2mjA3xmwXAnXoTqeUjpc1dC7vyUpTfuCyE0JqTfE9CBnBwx18FVLIXwMZdgGn6lh4gU9OzvjWLliMQYeNYOfdXy6BajyEhy/Zpo93na3nRs3rhtyWNnGI70h5AArVlw46/JiCG2NdMH3cF/k4AfBQJkVB6EpTbowqhQ4RsvFkLefy/UxFYjYRTa/fCAwJOwgSO4CQ4CRMgMIEJHgmzkOBi4PvEwIS9R8LNkeAk3EwAVe8kOAkTIDCBCR4JTsIECExggkeCk3AUAlt+jj6qzuPKGYuE9Or36nfyH9qd5ZnefXmcQftK+uMklO0qy6pZFAmXVSwsgO0D8Ihcf5iEstxIKgINrPgSav2alv6sc3+VBCjThhQEZ4E7Wu4fJQFRsK+rEeun7C2zFaxp7ZjqGBbppvEpi4fT9p+v8AyFpsjbbkvpgq/ZrNYYmL/xLOQtYtfVHYLTK4HjQyhlobyRs1RbrD0XGpvie5yheJMk7LAlqOjI9ut2DDZIgCRDNCp3UWg0ffItOcvbxZsIQl5+IUWCLCPvJiz0PZpehyIL56W+Ced5lSESt9hShe45X7aRsLyXV9jgr/bC0sjkeV/vEAwP5k6gnQMo2rWjbEEd531uJKE2pB03EEE8UsQx9+6enqjp41PZPH82RyPhKFtyJacdGCQUNVzx8tYGk4QhYWxRT7d9fwfUrSR8bUvb7qPPmCRkebna6GhGS4IGXsfkTZHQK1u7pp3bY0snz0GXbBIqbah+s9aSUAbJ1rtpBCXJOLN8EwklOuuy4xQ0Tln5GHKULczekw67JBSxbOrXmfC2/I5Uz442CWM3kpBnTem+I0xRMaVsxhBCrIwLEN6aIl3TlpOQZ8WukBBT0ouM6C0bBfj4tg5qmP6TRXuDpApjzYZ3uhKSvqeH24P6OtkQo7KOhGT9MiTSNW1haJ10OETCSXV7sQkBJ2ECV3ASnIQJEJjABI8EJ2ECBCYwIUcCHfjffRi4IHiCaHASnIQJEJjABI8EJ2ECBCYwwSPBSZgAgQlMOD8Szv6Z+OzyR0j60gabhPDbfdGchtcp6gsIfTO/NLBfeG+tejXncQk2thFyHKwS6iS8n3kRJwihSGiVthdDxuEWbDRwuFwkPLt81NP73GjDEAlxpUouXcZ1WydBYWMjCbIEJRI6y4sy98j3Lw1creLs8lcN+L5L7JDws/R0R8W2uPlTFunSAyTP95KFFQykIPzNa8S93Xm37/5bRyjJ9qVW1agv2Udt1cXArS0P0jMVACjjEta8X7Lefp2wWSGhMBsX0xmgvOJwXBQNcds0EtJGdQYqgIEgaV34i0iMgoK4S30RGuRt2WALAwCncp+bd/MlsYKwC4lT/mhfKwaGtCcIG2hbtReRK7rqqgy8W5rZvLIdnE5C2FGtDMaRjFZdkRsrKwnbGqdWZgOF4RDmCoEZypQbGxYJiyhHIYG2g6s0TsEUQ0pp2RfyJIcQNoK//GmVYbQRjglHNUmIFchQfLAuCh6rSU2yeVWosrNE89I+V6CdK7lAEIwPVzQSShZ2ZJTdzY889KxC7mBZmenQLAP5BUYi/QoJqI/IwD0DCjQ8C1nwKSrEaZ2ElTJTWVWUdMonzVSlJA/TbNifLDHzp+uf0pWFe6WX0j2bZRxKQjQI/WMcsFcAA9qbDFwpM5W1SgK7z8FYVD43kgAugggOKvOBLjHku4qEtefaNpFgGA0gtE1gm/JRBnWdvAvB+X0kwKdo1hUI5VPHxoacWulyy8QH3WrbHdFgEh70QEFAIHkpAx2RgcJyDnq2DABsNLBX5jNpT6t5QFO+FU2QUH5HQp4gnE5C+okiTzufGA9+lgpwjP6U/hGf4MFUMHcZDUigyvBMJuQNfXB4Osion4pSys9Eklb1/V5eNK1Em5gTBUuU/LAwipDZ83esS+IcaDbEMow2ijrbSKDcylP8MTp4eCdT5Q99nZu13LhwYBiIa2K3Xao/RxcvSDQoXmpvIKlriuRsiYSTxM3CZp0E3kg/Ph0BJ+F0iNcrcBLWMTo9hZNwOsTrFTgJ6xidnsJJOB3i9QoCCS4Gvk8MnFXZ61x5ijMR8O7oTHQHy3YSBoE6M5mTcCa6g2U7CYNAnZnMSTgT3cGynYRBoM5M5iScie5g2QMk4Hf/nyUv1AwW7snGEFgnIS1AxIVysSAyVseNqaylzhtNUqpeJSGuRj2WZ1rirJb1lALnOvUnSEiNIOSxnvyrWPgLJKSuKOKOsaHTJXGhVF5YT1qdirx27VZVeKhhJUW3VH4rvMVif9EbaXZoFQzaxrAZ2uGYpZfC4253JBfGVRki2oGxAyLcrHAggN5slzCQyVQMLG0lZ0HZ7DPbgHqYAqJWgsR3IgeVRU7L7WCF5sMNtuX2RiUIORF12VB1VLZQ+UgfnLMWHndIYF1RNtIOb0lYyJK6MG4QQKwCgxKPdndbxL6Qz6wJelP7NtmWQRVqPLRD1mmlXxb7ZXeWQRioa49NHiQrBggZcaR7Lu+wOyTtEIm/JM5qysge0DmAB8uu0naatrCNtiVQuYPFMg1bzPQmCaWg+KpzCu30l2ZJsnI1EpqKEyhyvODfh0iI725YFfvCCYbK3Ghb0zbQWrCzHiRBSnzq3RFCioMjj2XDklFx58f0MEXIw8O1eGYbCSkiYJn1uUXsu4eEr3cVPoiE2BVx8GpEotfzx6lSxTQ45ceh4hMrkKTGEgwD6+I735B/UOy7iQSULbs0w5xzI2HAGHh97uujhzePMin2Y6yR3VlIKncRbvKXSKoFmYbYV30QpSk0n9hk26kkNABnG9kB+k94DYiDfj990qa2jYYUeX+W1Z17WY04RBTiGbKu2DfMDGFLmkJmx0GJ/HODbWeSgEZ2bWWNQzp0YeGeIA3iNHBi3ox0scntDVEcS2r/5vCU4w1i35AJURLJUCOwFB4e4/puV2E4JBw0FW6SZs6OKqvWv3QqGJ7/r9fyZ1Pos6ONzUVfWnt7KqRH0MZ6/mryQ0jI3h6eTYgPZoSHM/JtvAjNv4rmznYdQwJV3jwsQn0w/UaiPO2409i/mu04Ev4qQhe0y0m4AOS1KgIJLgh2QfCao/z5694dTUCxk+AkTIDABCZ4JDgJEyAwgQkeCU7CBAhMYIIeCVhjVn8WncDqSUzAr8fraxR9g08mwVqO7Bs111W7DU7CZUzZJBxlgkfCKpKzkZBWyWioWBPBYq26L8ht15pVYTCrV4ppA4bs+ppdEfODRMXmqqHcyjRttFvrf6IpVAb9gtr8swbmVClti0bgdkWwYYPaV1zoVwW5WBAfEAaj3iQmCzvZ0y69kLjh+ohdQfZa7I8Kw7LtZjXIrrUh1VvlyTKbDTsc7yLhRwjDQJpU5XXEVxjUmgkYyuIXALKsF95jXUdZ0q6jRMUKCWhX+2hZ6tZkG6iMPSTUzBMS8Gq5lmz1pyl92HUSgmB8KsJgpbHAP3ya1y27qtzpi5XWakORuxc8rDJiFSCopE9l3EMCvCIJs6TOlb5z7zVBTviZ13ugHCAqburtEEampvRVlNC5W0n4Wny7k4SjRMW/m4SehyZg+UfTWH6xeFgV5iGJVg/OHSAqbuxC2bJbjvbe1B3ZRqkGAVspDG4ai4Tp07yu1W91GZBLSgC1Mux6MTVvHIIkQc2jApeMCdQNWoJcGDUgDDZBtsGIV3QAYdPXomLVrtKu4R2Ozx0TCAp4mCbIbW/WVGGw2thEQKiivE6Fne3M2g4SFZt2xfIhhsY9lbnDsUpC3RL/djIC+uzo5Eq9+BoBJ6HG45ZvTsItsNeVOgk1Hrd8cxJugb2uNJBAMyT/uw8Dj4TaKW/55iTcAntdqZNQ43HLNyfhFtjrSp2EGo9bvjkJt8BeV+ok1Hjc8s1J6MFu/lTdy7T92goJukgqLIa0b53eXvvOHN1VuZ1lqtnuJwGrUngyH+/WxHsw5TKg2oxTTv4zJGAJkOQZ7QY4JPPTzp+C+X2F3hoJqfJK+3MfFPfVfCcJiAKuROwikYxt06NLE10XS78m8O1dbxQNpARkry1uNjhh9dbt6dtZ16OPk896j9JYPKtPbUcyQhmYYZDQm9ZW199YZfUFlKWTEBXb9W65IX8qb+16DQ5XOWD8EuKCnXbyevJ4lEXOhpiYGrLWjgSWQgIaI4Cr0a2/7WwcearmQMX4/nUVHFlgUFYnc3fayeuhLYXaiWHCjEs3qcpMgtGOZNatJFSN47Qm48evGxHHy2SgbO02TTty+Ub9a+1I+RUSUGCfvVz/GY1bM765bnhiZWTxzO9JGBQTN3ZKg+J3hYSimmuN1QtB2LXpQajo2taM23z9QhK2iInX2pHgVEkAqMNTVLOyi0n4EWRLn/naTrRnUExs1lcbppOQH/r4WR7qTRndrL0WPK2USROhgJnEjwRnzbgd1zGtbvrvz3vJ8sNUbvV8AHt8at1OK+Ig9RROsNaObiTQRTxqxJ8Te9Je2PjZgo8Z8BB6uCNODx9hz2zaIZimiRuNWzNeu87tTTbkfbuzc3xvZyb7GXe97O5QrNlZB0H4ZkQCUiqCXWy7Kedp5HGZoLJZeTT6AhKC45BHwknwm5f4eeVrO9sbwuebcNrhbAnmFRJAhn+eiYCTcCa6g2U7CYNAnZnMSTgT3cGynYRBoM5M5iScie5g2YEEFwPfJwYm7D0SBr31zGROwpnoDpbtJAwCdWYyJ+FMdAfLdhIGgTozmZNwJrqDZTsJg0CdmcxJ6KE7uB7QK2Lk2goJutDJBcEj0I6n6ZDAVqHCQg4EVVg0EQs143V+nRLLps1S5tcliwLujgQs47kgmNbZW0m0oOurr3okJA8YVlt8ZcLEme+MBERBXh9fwykZ26ZHlya6LpZeFcoOXm89tF3/rd54yMqtm9S3s65HHyddEBwQTXKUsJ0nxi8XBNvbnyWPdEFwjEdlTIBHiS6kjt/62yFhzopM5dXdQO+60Z2wLOHwaDtz+Ub9a+1I+RUSUCAXd+Xa9IOjG7dmfHPdUsYJcw+z0wXBWfdfIuVCElwQnDzbigQptxSBAM1sIQ8JEP2iC27qQToXBCuRUOT8DcAuCHZBcNiYduWOWxmYEaL06YLgFsT2htAFwdxnfunxSiT80lb9MrOdhAkIcxKchAkQmMAEj4RZSHBBsAuCJ/DFe03w7uhe/EPtToKTMAECE5jgkeAkTIDABCZ4JDgJEyDQM6FZWesl3n9tJRJ0oZMLgvcDruXskID1VOyWAkGVC4I1IL85Z5IAKaQLgl0Q/I2DfZ/3zjEBUdAKfI12JWPb9OjSdCkJpXdB8KI90Q/gzlfgre0AvHa9lrdAvlm2gMP2azndTmfJ+dl+eQ/fIbh9/7L59I7vEIyo0rsj7mFVR7fWFzfXjXqqQo/cnFYWbNTf2Cnzxe/K7AgFnt8dHUfChVrUgJsLghUZ5IUkuCA4hXMT5hiURbcno7/JhwSIfpG/SY90LghWIsEFweyxqDI99B2Cy0TAHPtSICoDM0KUPl0Q7IJg7g9/+HglEv5wyydqmpMwARlOgpMwAQITmOCRMAsJLgh2QfAEvnivCd4d3Yt/qN1JcBImQGACEzwSnIQJEJjABI8EJ2ECBHomNCtrvcT7r61EgguC2/WE/WBbOTskYD3VBcFrK2MWuKPnTRIghXRBsAuCR53pnHR3jgmIglbga7Q1GdumR5emS0lcEBzxVLojAHe+Am9N8Lt2ve6roT0qig8XBAeSQageCb5DsBkJ8CgBnNEThdM7u6Pak1kFa31xc90gmxV5ip25fKP+xs6coTq4tTs6joQLtagBPhcEKzLIC0lwQXCK4ibMB7vQJh96hdHuBOlcEKxEgguCXRBML84If2IyY0YeIjB+KgMzT+CC4PYHPN8hmHvInzleiYQ/086pG+IkTECPk+AkTIDABCZ4JMxCgguCXRA8gS/ea4J3R/fiH2p3EpyECRCYwASPBCdhAgQmMMEjwUmYAIGeCYPrAb0iRq6tRIILgtv1hBFYt6XpkID1VBcEm6qQbVibqU0SIIV0QbALgk3vueTCnWMCoqAV+BpNT8a26dGl6QvgLgiOeCrdEYBzQbD2JvO8Ca7vEOw7BLd90s7uyJx1rPXFzXVEr+j2pKVH25nLN+pv7MwZqoNbu6PjSLhQixrgc0GwIoO8kAQXBKcobsLcBcHZM+nGjv/DTOLnR/TVDYg818Cmrkp+TKubLu7zXl7vT6wg5dtvpxVx7yU+mrWxnanZypiQrnxeyyMLXR/L4ylfbMSnsBiYyvNivkPwgDOtkhASuCC4/QHPBcHJd/7Wh90d/a12Tt0aJ2ECepwEJ2ECBCYwwSNhFhJcEOyC4Al88V4TvDu6F/9Qu5PgJEyAwAQmeCQ4CRMgMIEJHglOwgQI9ExQFo96yfdeW4kEFwS36wl7obbzdUhgq2U/2sqaWMqz6zj8CpZNm6XMo2u6OxKwZuuCYBcEH+3b28q7MxIQBUI8YTcgGdumR5cmui6W/v18pF2zHssLYo3B62131K7/Pp7vJWktsipkq511Pfo4+XzlWgpOa+1IKZUxAcBxNUUpVz1ildXXUZZOAg16tEFtUHI8WhLWrtfgQHtUFB++Q3BgY40Eg+hE6toOwpwEc7D+vJe3EmFbnIXXs3zey6dxekOPtNaOZIQSCfAo4b211fW3nZFQNY6XmMobv26Qzcuk46PtzOUb9a+1I+VXSECBhpfmitnB0Y1bM765bngiMzEcHmanC4KzR5dIuZAEFwQn17YiQWpejUgo5CEBol90wU09SOc7BCuRQKfiBrENwC4IfixRGLzmYfBIy9MHrnMB8wPi5bRzb74xgCeXaexW4XIm+/la3u/38no9mXB6YztTs5SBmTfYBcHtD3jtDeHzTTgR4aeQwAnx47MQWImEs6r1cjkCTgJH46ZjJ+Em4Hm1TgJH46ZjJ+Em4Hm1gQQXBLsgmDvFP3ns3dEEtDsJTsIECExggkeCkzABAhOY4JHgJEyAQM+EZmWtl3j/tZVI0IVOD1rQaHUf+63YmNOUt2wsZzX5/SSwVSgItGi7nQcUc2IBY7VFxyX4Z0jAMp4Lgl0QfFz47Cnpzu4IUZDXx9cakIxt06NLE10XS++C4GVRBmYAd74Cb03wu3a9lrdAvlmUFC4IDtEDQvVIWBP8rl3nJJiDtQuC+yRwEKseb60vbq4b9VSFuiB4gTo94NKAKNDafP1CLWow1QXBigzyQhJcEJwipokUDMpi7BEBhucT2m7Q6M6aepDOBcFKJLggeFkWeEiZHm4V2lZO23hgdVUlYXFBMPHwXl75d6Wf5fF4LrRF9S6h7B4SiKcP7VmN37bwUCJ7ejOl+c5OFwSLkPgbX5U75r/RsN/UCidhAracBCdhAgQmMMEjYRYSXBDsguAJfPFeE7w7uhf/ULuT4CRMgMAEJngkOAkTIDCBCR4JTsIECPRMWPtJvZd3w7WVSHBBcLvByAZ0B5N2SGCrZS4IHoRzXzKTBEghXRDsguB9rnVUrjvHBERBK/A1WpeMbdOjSxNSFJbeBcEuCE5e1XcWrlPKmtfHc3m9aePbV3ohttJtJWczhc2pdmVMGBRS8aBgns1PFymMHglrgt+16yo4cs9qFwSPe1hFXiKVg9y/mjPV0QAAD0tJREFUbtRTZXJBsAuCpUOw70p3BMHWz9IOtCwnP9zZHY17Oq+seHTJ74JgXZYYcDO6ic3dzSAJcktMkc0FwRyQw0ko0VuiI1XoOwT7DsEjvz2pY0JxWt8huAXRBcHFP/7Q0Uok/KGWTtwUJ2ECcpwEJ2ECBCYwwSNhFhJcEOyC4Al88V4TvDu6F/9Qu5PgJEyAwAQmeCQ4CRMgMIEJHglOwgQI9ExYW3zq5d1wbSUSXBDcridsQHcwaYcErA9jtxS8zxK7qAgt0WCFRyTLAiypMTqicF7G3ZEAKaQLghVlHSfqgGM9EpIH/DxeS/M6+gMq/TVF3BkJiIKzdUdUvguCXRCcghLjnxjnlEjI45ELgp+LnLVkcORg7YLgcQ+rxgnFA/vXjXqqTEU+2XazRv41O3L53+VXBmYUeP6G5Y1aDo1aa3xz/UItarDRdwhWtK8XkuA7BKdQsSLBBcE/C93Y8X8YLH8kOA2IPFfpu7d0V5hWN3lcEOyCYDmLE+4WvioDM0/mguAWRBcEcw/5M8crkfBn2jl1Q5yECehxEpyECRCYwASPhFlIcEGwC4In8MV7TfDu6F78Q+1OgpMwAQITmOCR4CRMgMAEJngkOAkTINAzYW3xqZd3w7WVSHBBcLuesAHdwaQdEqC6cEFws2Q6CO5oMpMErNm6INgFwaPOdE66O8cERIEQT9gNTca26dGl6RpPSu+CYBcEJ8fqOwsfEyDjoVcd+w7BP3VfncFxQbDsscY9rMq51hc31416qkKLqGxrt8kjQRYZvxv1N3bquZXZEQp0QXBvH9XP+7W8nuk5vudzefzQVF4f+9ZIVEgoe4y2HqMzCWPb9CB0o3FrHtRcd0GwopIGWReTID0RZuCzIQ8XRu1EusfyfPOn+XB+o7Ol6tVIKFvu0+D3Vh4e/Czv16tsQp4a54LgQ0lYluXzSv2c9bMFHzPgCT/LzyP2k49Hyhc+NxpnemxyHe06tzfZ8Ax1c7X493biHurxfC3v93t5vTAeHDwmIEgpIuhmKg46VAkIeS3vDw9HIu29vB540PxnoXk0RWw0+gISyOjPe3kyG+glGI+niOSv7XRBcPGPP3RkjAl/qIW/oClOwgQkOQlOwgQITGCCR8IsJLgg2AXBE/jivSZ4d3Qv/qF2J8FJmACBCUzwSHASJkBgAhM8EpyECRDomaCtW/TS77y2EgkuCHZBsOFZpsbISL/79N2RgGU8FwTXIrPdhHYy6t1R8gDfIbjd+rOD5e5LKgmIglZHZNSTSGvTY2FdX2Om9C4IdkFw8qq+s3AFXR6PXBDcdhMZHBcEy+5q3MOqnGuzkua6UU9VqAuCi1qPgGlAFGhtvn6hFjWY6jsEKyReSILvEJwipomURIILgrnmM4KFwXKzbr8Beb27wrSaz2Riz/FeXlBQp3L3C5etiCP55RlaVC6wDZpO+WIjFwS7IFg8swavd0Gw6DX86xgC6s8WY1k91VEIOAlHIflFOU7CF+AdldVJOArJL8pxEr4A76isgQQXBLsg+CiH+rXleHc0AXVOgpMwAQITmOCR4CRMgMAEJngkOAkTINAzYW1xqZd3w7WVSHBBsAuCDW/CsmmzlGmk33367kjAmq0Lgl0QvNuJD8l4ZyQgClqBr9G0ZGyb3lDGsfQuCHZBcPKqvrPwsSePRy4IdkFw2yex7qW+OO5hVb61vri5btRTFVo0sFu7TR4Jssj43ai/sVPPrdwnoEAu7tIz57O3k2Ap47KF8eAwO10Q7IJg4VuVh7Vhi6jSH5dq06fS18K4ue6C4OyZ+4W2gtoG5PXrmFY3xNI+qC4I3qFW3kFCtaOx7xAcdxL2HYLLbKyJThHYyuxIpPCvpyPgJJwO8XoFTsI6RqencBJOh3i9AidhHaPTUzgJp0O8XkEgwQXBLghed5U/nsK7owkIdhKchAkQmMAEjwQnYQIEJjDBI8FJmACBNRPW1jbW8g9c1yMBO7y0soRcJPQ3nSQ57a8+cBImoM9JcBIWuXDPIfHuiKPx3fGxY0IKXRonPtWGfY/lKfcrzXa3b7rF+nROQge9sqGm+LzSNmjxbblqnaGcKGwLNuJ1wbSJlXxjLau3XSemd1Lz1wDTa5Dly7LJ7Ljmro6dqU2nkEDGhCdcguqhvEpYa8grgECv7I3vNy4NEwpAGIyyn7QdHHv1MI7xPujw2mLl2YJEwgPliPRN9Kd6a9uhp0rAJ1vwymQiM//rTHIiQY/lHBJ+BIAw5PGq3nJudmlIzxsDEkTZKKN5rAlliDoRUfSu5jo4ISAT5xUScp3cvoC6VgYIEwK4pUg3TyGh9hqyTjMknQsvz/4snw//S7srcgAVMEK7AXYDiFZn6dZaG+ll7CmCOTtNvUa5yfW1MrRzcAay40YS4DWxzyRPbv5GSIBHHUACByZhmseiQlrxYPFu9pgFEVvZ0+ZBV0R830/CI75qvo6EFBUZiZ4HpwZWjaaMhsc2ns0q0a4151pAWQmZNDm2cNAX4Tg6CUhk7ri7tYFaeu1c1Zz6SwMGLh9HArqNis+m3r7dKKNETrKTl5OOUY9BQplaEaMy7FCRZFsN52CDbjjKaQymPGGMANDHR8I3AzOmnY3dn+QQzaBP7UAE0ayOut4yUJskhEx5Dv1YnmEaxubFNKAyjMIhZ7u6ppOQDQtz9Nfyer+X9/u1vNJ0s2qkWfa+SMD4Q/ckNNXN00s+KAfs2kezuN3h3kBOl2UZCQs4XagbYbBYY0IG0LghebXREbKYQFkkUK72Zu0nPJQn4s8sex8JRPCnutlqb7RW28TII2CJ0CzDzxiyA8zkRKR0IoFl/kuHJplXNBJdUn2/5CRcgT3qMBzASQBAp3+iSxZ35OtjwumWXV+B4Y2nG4LxgN+Apkr/vUg4He3tFTgJ2zE7PEcgwQXBLgg+3LN+W4HeHU3AmJPgJEyAwAQmeCQ4CRMgMIEJHgn/Bgnxp2r8fv/zTL8g3vXzwQSgSxP0SMDvHGzhQWYc/Z4XMmgtOSzYnE8C6qwWhUYNviHdySTgl8Mtq3CjKOi/zVNuJ6HCMJGg/HJor0dXBXS+2CR0Mk156ZpIcBK65G8jIQ2mNFSsCX6hSMgDchJ35WFGHZj1XeqlsNcqO6s/1LIJB2PNHIJiDtWGtvJs1TErQ90JeYnt3UXCkOD3E5UT4WXReTfdNylZ4j8FqNyX5/RFZV0NsqnsoJDIad9LxlIpu4jBBkS8ZGEqY6itFfLsC8oIDkii5+dCryp+JZkK2ruPBKEWyPvPNd3OxjHhw0jKbbH6fut8AZATlxucQxEVpHJkmzKAYjkSM8emrSiPfVplIElq7y4SeONiedYsaCMJMK76tMreQoJVRrJ+SAgMo/plIVX4TCS0eFWpNmpRzUItw9J5zWs6ZX3C/UR6Hf0TgjM5zd1CQict4QGP5VHSsS8+UyHtqYEdJ2GrKnuzYRtJqJ7ukSpt2egOsI2dnbR3koD20tJm8w/9HvcMZmwbXkdEAsqQSjic/4YEq4zYcowXVbsaIoFSvyykCp9mGXQV5Wx9UscsFAVKoNL5oe7I8tb0wAgT0MaGWnWW7oWDimktPxfKsUS8m9sarar+N8ugVKW9Uw3MGaj0/Nqr0opKgplyHKJeRK7a+NRomi7SA35rIl61DAKvQ37FgO4MPAnaOxUJoYF4ADDNrZ9v+hW2lpKXhiBK4viRvbwH4KiIt1dGUKu3TlHsSkdmGUj5iQp0dUxAGv+8BAE9Ei6p2isBAk4CkLjx00m4EXxU7SQAiRs/nYQbwUfVgQQXBLsgGA7xT35SAHh3dDP1TsLNBFD1ToKTMAECE5jgkeAkTIDABCZ4JPwaEujdleI3eNqmRt1MA8uiSeiVhV+PtNEgNEey8UY+tQ6Z95d/X42Eohqj1aiifiABEwDGYlbAAmDmlauYB9vYUJ688MLB0/KxOtQ8PP8vPu6SkJfewvZoWivji6Gz6o2SAMyKmZiXZCwgo7ls5cP5Zn1Zs+d3nrNJ2Nt45GtQTgCl5T6+61W40skHZ7CK/J3QF6tNEnY3vANmrBaL5D9LBWonH+QoVfrShl9/ZJAAoAYWsiUEHTCRFKBW/Xwn326HQIWTfxokQBoiSABQ1czHEMv23FZTIKBsmc/qviYHdot520hYaKpaZkiPtElhhZsFJrOqGwl8VsU3QWx2PGQF/vLDjSTUrVW7iQ0kqOSJKAubmFv3FrU5v/abQUJRtlVAiWbuIwHjzY5uTNT/V76aJGSpuKYhTa3fRQIiRZaL8z3W/wrqoh02CUFGmaTpxs3aZhIA9I+YnpJRuOYkCIqy8BVkFBEt7nybrY4BJh9g80MeVI7xRhHkcxIkCfE7/dxAvxUV4KOqWR00AWY1wCYVNO0qbA2yyOck6CT42XMR6I4J51btpQMBJwFI3PjpJNwIPqp2EoDEjZ9Owo3go+pMAh34330YuBYVLnnTZ46Em+r3al2LOocPeCRMwIOT4CRMgMAEJngkOAkTIDCBCf1IoC1omB407IxSaR4naMEfMMEmATtShXdi1qLeSrTF9uyx1mvGcCp7/3xXzlhtM6UySYjrx0IREZaCH0JVfRR4R5UzE7xjthgkQJYiFHhqmUeBd1Q5qpFTn1whQVFFsOZAbYHnFPJnXic+drffXGy2wXKWdhfgBylGcr65DgwSyrZh4aEO5aXYoRkru/RmuWPewfe73X5HSYBzBCECvd+ZtnCjXXknHWxsEqj/Z4NzJONVtkHOztTpRg7e7XeMhBQdUlyW7Z3voEtCNFeGNm2XyRvSIYEny8dWF9IpJymz63qpQK0snKP9qCd1/YxFPBggATmIDDynxgfsDngha3ykqlJzB00SL4MSdsrZREJU84VN0qkeEqFZb1FH027+3EBCtBT9bfHKPniVYKwShJ1IQgI1itaSelBuRn4z8Lz6E0ko3QJtqVn+4fx2EuqbRCrRKqvUFlLhQZPiOXWCm7/pJJAk8fFsB+G8ky4H0ALCihDsY8rLCFAt5ibgBoiYfTUPITagJlt+HQnoOvKzyxgP2ueQ0UXF55zpofE4cuM8poq7d/vNHl+epY5PCdED7dTdMEKDpjU9uE7T06CjjV3SpBx0ttpRnuKP0cG7FrgcvDs2tnQbafdbEEqK7D27/YZAeS8v9mMiIjUSzUig17XI3QfCfYpmN+y/91Pvju616Z+r3UmYgHInwUmYAIEJTPBIcBImQGACE3Ik0IH/3YfB/wFKPS3D+ghE/wAAAABJRU5ErkJggg=="
    }
   },
   "cell_type": "markdown",
   "id": "e55a2da1",
   "metadata": {},
   "source": [
    "### 3. Dataset: Predict students' dropout and academic success \n",
    "Data link:https://archive.ics.uci.edu/dataset/697/predict+students+dropout+and+academic+success\n",
    "Data size:  4424 x36\n",
    " This is a multiclassification case where the target is student's status and has 3 values: \"Dropout, \"Enrolled\" and \"Graduate\". \n",
    " \n",
    "**Variables:**\n",
    "*Describtions can be found in the website, it is too long to copy here*\n",
    " Marital status\n",
    "Application mode\n",
    "Application order\n",
    "Course\n",
    "\"Daytime/evening attendance\t\"\n",
    "Previous qualification\n",
    "Previous qualification (grade)\n",
    "Nacionality\n",
    "Mother's qualification\n",
    "Father's qualification\n",
    "Mother's occupation\n",
    "Father's occupation\n",
    "Admission grade\n",
    "Displaced\n",
    "Educational special needs\n",
    "Debtor\n",
    "Tuition fees up to date\n",
    "Gender\n",
    "Scholarship holder\n",
    "Age at enrollment\n",
    "International\n",
    "Curricular units 1st sem (credited)\n",
    "Curricular units 1st sem (enrolled)\n",
    "Curricular units 1st sem (evaluations)\n",
    "Curricular units 1st sem (approved)\n",
    "Curricular units 1st sem (grade)\n",
    "Curricular units 1st sem (without evaluations)\n",
    "Curricular units 2nd sem (credited)\n",
    "Curricular units 2nd sem (enrolled)\n",
    "Curricular units 2nd sem (evaluations)\n",
    "Curricular units 2nd sem (approved)\n",
    "Curricular units 2nd sem (grade)\n",
    "Curricular units 2nd sem (without evaluations)\n",
    "Unemployment rate\n",
    "Inflation rate\n",
    "GDP\n",
    "Status\n",
    "![image.png](attachment:image.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1c691d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming 'Target' contains your categorical labels\n",
    "# Replace 'data['Target']' with your actual target column\n",
    "target_values = [\"dropout\", \"enrolled\", \"graduate\"]  # Define your categorical values in order\n",
    "\n",
    "# Creating a LabelEncoder instance\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit LabelEncoder and transform the target column\n",
    "data['Status'] = label_encoder.fit_transform(data['Status'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "47e477a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With 1 neighbors, Euclidean Distance, Average Accuracy: 0.5535\n",
      "With 2 neighbors, Euclidean Distance, Average Accuracy: 0.5230\n",
      "With 3 neighbors, Euclidean Distance, Average Accuracy: 0.5829\n",
      "With 4 neighbors, Euclidean Distance, Average Accuracy: 0.5793\n",
      "With 5 neighbors, Euclidean Distance, Average Accuracy: 0.5894\n",
      "With 6 neighbors, Euclidean Distance, Average Accuracy: 0.5965\n",
      "With 7 neighbors, Euclidean Distance, Average Accuracy: 0.6027\n",
      "With 8 neighbors, Euclidean Distance, Average Accuracy: 0.6081\n",
      "With 9 neighbors, Euclidean Distance, Average Accuracy: 0.6041\n",
      "With 10 neighbors, Euclidean Distance, Average Accuracy: 0.6078\n",
      "With 11 neighbors, Euclidean Distance, Average Accuracy: 0.6033\n",
      "With 12 neighbors, Euclidean Distance, Average Accuracy: 0.6075\n",
      "With 13 neighbors, Euclidean Distance, Average Accuracy: 0.6007\n",
      "With 14 neighbors, Euclidean Distance, Average Accuracy: 0.6016\n",
      "With 15 neighbors, Euclidean Distance, Average Accuracy: 0.6024\n",
      "With 16 neighbors, Euclidean Distance, Average Accuracy: 0.5999\n",
      "With 17 neighbors, Euclidean Distance, Average Accuracy: 0.6010\n",
      "With 18 neighbors, Euclidean Distance, Average Accuracy: 0.5993\n",
      "With 19 neighbors, Euclidean Distance, Average Accuracy: 0.5999\n",
      "With 20 neighbors, Euclidean Distance, Average Accuracy: 0.5996\n",
      "With 1 neighbors, Manhattan Distance, Average Accuracy: 0.6078\n",
      "With 2 neighbors, Manhattan Distance, Average Accuracy: 0.5793\n",
      "With 3 neighbors, Manhattan Distance, Average Accuracy: 0.6400\n",
      "With 4 neighbors, Manhattan Distance, Average Accuracy: 0.6403\n",
      "With 5 neighbors, Manhattan Distance, Average Accuracy: 0.6434\n",
      "With 6 neighbors, Manhattan Distance, Average Accuracy: 0.6474\n",
      "With 7 neighbors, Manhattan Distance, Average Accuracy: 0.6516\n",
      "With 8 neighbors, Manhattan Distance, Average Accuracy: 0.6558\n",
      "With 9 neighbors, Manhattan Distance, Average Accuracy: 0.6567\n",
      "With 10 neighbors, Manhattan Distance, Average Accuracy: 0.6587\n",
      "With 11 neighbors, Manhattan Distance, Average Accuracy: 0.6604\n",
      "With 12 neighbors, Manhattan Distance, Average Accuracy: 0.6626\n",
      "With 13 neighbors, Manhattan Distance, Average Accuracy: 0.6564\n",
      "With 14 neighbors, Manhattan Distance, Average Accuracy: 0.6589\n",
      "With 15 neighbors, Manhattan Distance, Average Accuracy: 0.6595\n",
      "With 16 neighbors, Manhattan Distance, Average Accuracy: 0.6570\n",
      "With 17 neighbors, Manhattan Distance, Average Accuracy: 0.6567\n",
      "With 18 neighbors, Manhattan Distance, Average Accuracy: 0.6626\n",
      "With 19 neighbors, Manhattan Distance, Average Accuracy: 0.6601\n",
      "With 20 neighbors, Manhattan Distance, Average Accuracy: 0.6612\n",
      "Test Accuracy with best parameters (Distance: Manhattan, Neighbors: 18): 0.6418\n"
     ]
    }
   ],
   "source": [
    "#Student dropout NN\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming your data is loaded into a DataFrame named 'data'\n",
    "# Extracting features (X) and target (y)\n",
    "X = data.drop('Status', axis=1)  # Replace 'target_column_name' with your target column\n",
    "y = data['Status']  # Replace 'target_column_name' with your target column\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# List of n_neighbors values to experiment with\n",
    "n_neighbors_values = list(range(1, 21))  # Adjust or expand as needed\n",
    "distances = ['euclidean', 'manhattan']\n",
    "\n",
    "best_accuracy = 0\n",
    "best_n_neighbors = None\n",
    "best_distance = None\n",
    "\n",
    "for distance in distances:\n",
    "    for n_neighbors in n_neighbors_values:\n",
    "        # Create K-Nearest Neighbors classifier\n",
    "        knn = KNeighborsClassifier(n_neighbors=n_neighbors, metric=distance)\n",
    "        \n",
    "        # Perform cross-validation on the training data\n",
    "        scores = cross_val_score(knn, X_train, y_train, cv=5)\n",
    "        \n",
    "        # Calculate average accuracy across folds\n",
    "        avg_accuracy = scores.mean()\n",
    "        \n",
    "        print(f\"With {n_neighbors} neighbors, {distance.capitalize()} Distance, Average Accuracy: {avg_accuracy:.4f}\")\n",
    "        \n",
    "        # Check if this model is the best so far\n",
    "        if avg_accuracy > best_accuracy:\n",
    "            best_accuracy = avg_accuracy\n",
    "            best_n_neighbors = n_neighbors\n",
    "            best_distance = distance\n",
    "\n",
    "# Now, train the model on the full training set with the best parameters found\n",
    "best_knn = KNeighborsClassifier(n_neighbors=best_n_neighbors, metric=best_distance)\n",
    "best_knn.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on the separate test set\n",
    "test_accuracy = best_knn.score(X_test, y_test)\n",
    "print(f\"Test Accuracy with best parameters (Distance: {best_distance.capitalize()}, Neighbors: {best_n_neighbors}): {test_accuracy:.4f}\")\n",
    "\n",
    "\n",
    "#best: With 12 neighbors, Manhattan Distance, Average Accuracy: 0.6626 & With 18 neighbors, Manhattan Distance, Average Accuracy: 0.6626"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "fbce7a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With min_samples_leaf=1, Average Accuracy: 0.6844\n",
      "With min_samples_leaf=5, Average Accuracy: 0.7152\n",
      "With min_samples_leaf=10, Average Accuracy: 0.7234\n",
      "With min_samples_leaf=20, Average Accuracy: 0.7479\n",
      "With min_samples_leaf=25, Average Accuracy: 0.7550\n",
      "With min_samples_leaf=30, Average Accuracy: 0.7494\n",
      "Test Accuracy with best min_samples_leaf=25: 0.7311\n"
     ]
    }
   ],
   "source": [
    "#Student dropout Decision tree\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "# List of min_samples_leaf values to experiment with\n",
    "min_samples_leaf_values = [1, 5, 10, 20, 25, 30]  # Adjust as needed\n",
    "\n",
    "best_accuracy = 0\n",
    "best_min_samples_leaf = None\n",
    "\n",
    "for min_samples_leaf in min_samples_leaf_values:\n",
    "    # Create Decision Tree Classifier\n",
    "    clf = DecisionTreeClassifier(min_samples_leaf=min_samples_leaf, random_state=42)\n",
    "    \n",
    "    # Perform cross-validation on the training data\n",
    "    scores = cross_val_score(clf, X_train, y_train, cv=5)\n",
    "    \n",
    "    # Calculate average accuracy across folds\n",
    "    avg_accuracy = scores.mean()\n",
    "    \n",
    "    print(f\"With min_samples_leaf={min_samples_leaf}, Average Accuracy: {avg_accuracy:.4f}\")\n",
    "    \n",
    "    # Check if this model is the best so far\n",
    "    if avg_accuracy > best_accuracy:\n",
    "        best_accuracy = avg_accuracy\n",
    "        best_min_samples_leaf = min_samples_leaf\n",
    "\n",
    "# Now, train the model on the full training set with the best parameter found\n",
    "best_clf = DecisionTreeClassifier(min_samples_leaf=best_min_samples_leaf, random_state=42)\n",
    "best_clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on the separate test set\n",
    "test_accuracy = best_clf.score(X_test, y_test)\n",
    "print(f\"Test Accuracy with best min_samples_leaf={best_min_samples_leaf}: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e003ae0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With max_features=0.3, Average Accuracy: 0.7708\n",
      "With max_features=0.4, Average Accuracy: 0.7706\n",
      "With max_features=0.5, Average Accuracy: 0.7742\n",
      "With max_features=0.6, Average Accuracy: 0.7751\n",
      "With max_features=0.7, Average Accuracy: 0.7754\n",
      "Test Accuracy with best max_features=0.7: 0.7571\n"
     ]
    }
   ],
   "source": [
    "#Student dropout RF\n",
    "\n",
    "# List of max_features values to experiment with\n",
    "max_features_values = [ 0.3, 0.4, 0.5, 0.6, 0.7]  # Adjust or expand as needed\n",
    "\n",
    "best_accuracy = 0\n",
    "best_max_features = None\n",
    "\n",
    "for max_feature in max_features_values:\n",
    "    # Create Random Forest Classifier\n",
    "    rf = RandomForestClassifier(n_estimators=500, min_samples_leaf=5, max_features=max_feature, random_state=42)\n",
    "    \n",
    "    # Perform cross-validation on the training data\n",
    "    scores = cross_val_score(rf, X_train, y_train, cv=5)\n",
    "    \n",
    "    # Calculate average accuracy across folds\n",
    "    avg_accuracy = scores.mean()\n",
    "    \n",
    "    print(f\"With max_features={max_feature}, Average Accuracy: {avg_accuracy:.4f}\")\n",
    "    \n",
    "    # Check if this model is the best so far\n",
    "    if avg_accuracy > best_accuracy:\n",
    "        best_accuracy = avg_accuracy\n",
    "        best_max_features = max_feature\n",
    "\n",
    "# Train the model on the full training set with the best max_features found\n",
    "best_rf = RandomForestClassifier(n_estimators=500, min_samples_leaf=5, max_features=best_max_features, random_state=42)\n",
    "best_rf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on the separate test set\n",
    "test_accuracy = best_rf.score(X_test, y_test)\n",
    "print(f\"Test Accuracy with best max_features={best_max_features}: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "9c6bb2c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 200}\n",
      "Best Cross-validation Accuracy: 0.7858153733048849\n",
      "Test Set Accuracy: 0.7694915254237288\n"
     ]
    }
   ],
   "source": [
    "#Student dropout GBT\n",
    "\n",
    "# Define the parameter grid for tuning\n",
    "param_grid = {\n",
    "    'learning_rate': [0.1, 0.2, 0.3],  # Learning rate values to try\n",
    "    'n_estimators': [100, 200],  # Number of trees to try\n",
    "    'max_depth': [3, 5, 7]  # Depth of trees to try\n",
    "}\n",
    "\n",
    "#since the data is large, code runs for too long when i tried for 5 alternatives each\n",
    "\n",
    "# Create the Gradient Boosting Classifier\n",
    "gbt = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Use GridSearchCV to search for the best parameters\n",
    "grid_search = GridSearchCV(gbt, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)  # Using the training set for tuning\n",
    "\n",
    "# Get the best parameters and the best score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best Cross-validation Accuracy:\", best_score)\n",
    "\n",
    "# Get the best model based on the best parameters\n",
    "best_gbt = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "test_accuracy = best_gbt.score(X_test, y_test)\n",
    "print(\"Test Set Accuracy:\", test_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0befef",
   "metadata": {},
   "source": [
    "In all of the methods, test results are worse by nearly 0.02. It might be a case of overfitting. The best resukts are obtained with GBT method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931ca496",
   "metadata": {},
   "source": [
    "### 4. Dataset Breast Cancer Diagnosis\n",
    "\n",
    "Data link:ttps://archive.ics.uci.edu/dataset/17/breast+cancer+wisconsin+diagnostic \n",
    "Data size: 569 x30 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "fcae61f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ucimlrepo in c:\\users\\dell\\anaconda3\\lib\\site-packages (0.0.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install ucimlrepo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b6c4af1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'uci_id': 17, 'name': 'Breast Cancer Wisconsin (Diagnostic)', 'repository_url': 'https://archive.ics.uci.edu/dataset/17/breast+cancer+wisconsin+diagnostic', 'data_url': 'https://archive.ics.uci.edu/static/public/17/data.csv', 'abstract': 'Diagnostic Wisconsin Breast Cancer Database.', 'area': 'Health and Medicine', 'tasks': ['Classification'], 'characteristics': ['Multivariate'], 'num_instances': 569, 'num_features': 30, 'feature_types': ['Real'], 'demographics': [], 'target_col': ['Diagnosis'], 'index_col': ['ID'], 'has_missing_values': 'no', 'missing_values_symbol': None, 'year_of_dataset_creation': 1993, 'last_updated': 'Fri Nov 03 2023', 'dataset_doi': '10.24432/C5DW2B', 'creators': ['William Wolberg', 'Olvi Mangasarian', 'Nick Street', 'W. Street'], 'intro_paper': {'title': 'Nuclear feature extraction for breast tumor diagnosis', 'authors': 'W. Street, W. Wolberg, O. Mangasarian', 'published_in': 'Electronic imaging', 'year': 1993, 'url': 'https://www.semanticscholar.org/paper/53f0fbb425bc14468eb3bf96b2e1d41ba8087f36', 'doi': '10.1117/12.148698'}, 'additional_info': {'summary': 'Features are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass.  They describe characteristics of the cell nuclei present in the image. A few of the images can be found at http://www.cs.wisc.edu/~street/images/\\r\\n\\r\\nSeparating plane described above was obtained using Multisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree Construction Via Linear Programming.\" Proceedings of the 4th Midwest Artificial Intelligence and Cognitive Science Society, pp. 97-101, 1992], a classification method which uses linear programming to construct a decision tree.  Relevant features were selected using an exhaustive search in the space of 1-4 features and 1-3 separating planes.\\r\\n\\r\\nThe actual linear program used to obtain the separating plane in the 3-dimensional space is that described in: [K. P. Bennett and O. L. Mangasarian: \"Robust Linear Programming Discrimination of Two Linearly Inseparable Sets\", Optimization Methods and Software 1, 1992, 23-34].\\r\\n\\r\\nThis database is also available through the UW CS ftp server:\\r\\nftp ftp.cs.wisc.edu\\r\\ncd math-prog/cpo-dataset/machine-learn/WDBC/', 'purpose': None, 'funded_by': None, 'instances_represent': None, 'recommended_data_splits': None, 'sensitive_data': None, 'preprocessing_description': None, 'variable_info': '1) ID number\\r\\n2) Diagnosis (M = malignant, B = benign)\\r\\n3-32)\\r\\n\\r\\nTen real-valued features are computed for each cell nucleus:\\r\\n\\r\\n\\ta) radius (mean of distances from center to points on the perimeter)\\r\\n\\tb) texture (standard deviation of gray-scale values)\\r\\n\\tc) perimeter\\r\\n\\td) area\\r\\n\\te) smoothness (local variation in radius lengths)\\r\\n\\tf) compactness (perimeter^2 / area - 1.0)\\r\\n\\tg) concavity (severity of concave portions of the contour)\\r\\n\\th) concave points (number of concave portions of the contour)\\r\\n\\ti) symmetry \\r\\n\\tj) fractal dimension (\"coastline approximation\" - 1)', 'citation': None}}\n",
      "                  name     role         type demographic description units  \\\n",
      "0                   ID       ID  Categorical        None        None  None   \n",
      "1            Diagnosis   Target  Categorical        None        None  None   \n",
      "2              radius1  Feature   Continuous        None        None  None   \n",
      "3             texture1  Feature   Continuous        None        None  None   \n",
      "4           perimeter1  Feature   Continuous        None        None  None   \n",
      "5                area1  Feature   Continuous        None        None  None   \n",
      "6          smoothness1  Feature   Continuous        None        None  None   \n",
      "7         compactness1  Feature   Continuous        None        None  None   \n",
      "8           concavity1  Feature   Continuous        None        None  None   \n",
      "9      concave_points1  Feature   Continuous        None        None  None   \n",
      "10           symmetry1  Feature   Continuous        None        None  None   \n",
      "11  fractal_dimension1  Feature   Continuous        None        None  None   \n",
      "12             radius2  Feature   Continuous        None        None  None   \n",
      "13            texture2  Feature   Continuous        None        None  None   \n",
      "14          perimeter2  Feature   Continuous        None        None  None   \n",
      "15               area2  Feature   Continuous        None        None  None   \n",
      "16         smoothness2  Feature   Continuous        None        None  None   \n",
      "17        compactness2  Feature   Continuous        None        None  None   \n",
      "18          concavity2  Feature   Continuous        None        None  None   \n",
      "19     concave_points2  Feature   Continuous        None        None  None   \n",
      "20           symmetry2  Feature   Continuous        None        None  None   \n",
      "21  fractal_dimension2  Feature   Continuous        None        None  None   \n",
      "22             radius3  Feature   Continuous        None        None  None   \n",
      "23            texture3  Feature   Continuous        None        None  None   \n",
      "24          perimeter3  Feature   Continuous        None        None  None   \n",
      "25               area3  Feature   Continuous        None        None  None   \n",
      "26         smoothness3  Feature   Continuous        None        None  None   \n",
      "27        compactness3  Feature   Continuous        None        None  None   \n",
      "28          concavity3  Feature   Continuous        None        None  None   \n",
      "29     concave_points3  Feature   Continuous        None        None  None   \n",
      "30           symmetry3  Feature   Continuous        None        None  None   \n",
      "31  fractal_dimension3  Feature   Continuous        None        None  None   \n",
      "\n",
      "   missing_values  \n",
      "0              no  \n",
      "1              no  \n",
      "2              no  \n",
      "3              no  \n",
      "4              no  \n",
      "5              no  \n",
      "6              no  \n",
      "7              no  \n",
      "8              no  \n",
      "9              no  \n",
      "10             no  \n",
      "11             no  \n",
      "12             no  \n",
      "13             no  \n",
      "14             no  \n",
      "15             no  \n",
      "16             no  \n",
      "17             no  \n",
      "18             no  \n",
      "19             no  \n",
      "20             no  \n",
      "21             no  \n",
      "22             no  \n",
      "23             no  \n",
      "24             no  \n",
      "25             no  \n",
      "26             no  \n",
      "27             no  \n",
      "28             no  \n",
      "29             no  \n",
      "30             no  \n",
      "31             no  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "from ucimlrepo import fetch_ucirepo \n",
    "  \n",
    "# fetch dataset \n",
    "breast_cancer_wisconsin_diagnostic = fetch_ucirepo(id=17) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = breast_cancer_wisconsin_diagnostic.data.features \n",
    "y = breast_cancer_wisconsin_diagnostic.data.targets \n",
    "  \n",
    "# metadata \n",
    "print(breast_cancer_wisconsin_diagnostic.metadata) \n",
    "  \n",
    "# variable information \n",
    "print(breast_cancer_wisconsin_diagnostic.variables) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "43a394c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "# Assuming 'y' contains your categorical labels 'M' and 'B'\n",
    "# Replace 'y' with your actual target variable\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "y = np.ravel(y)\n",
    "\n",
    "# Fit LabelEncoder and transform the target column\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "# Now 'y_encoded' contains the encoded values (0 and 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "811a42b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With 1 neighbors, Euclidean Distance, Average Accuracy: 0.9143\n",
      "With 2 neighbors, Euclidean Distance, Average Accuracy: 0.9209\n",
      "With 3 neighbors, Euclidean Distance, Average Accuracy: 0.9209\n",
      "With 4 neighbors, Euclidean Distance, Average Accuracy: 0.9209\n",
      "With 5 neighbors, Euclidean Distance, Average Accuracy: 0.9231\n",
      "With 6 neighbors, Euclidean Distance, Average Accuracy: 0.9143\n",
      "With 7 neighbors, Euclidean Distance, Average Accuracy: 0.9253\n",
      "With 8 neighbors, Euclidean Distance, Average Accuracy: 0.9165\n",
      "With 9 neighbors, Euclidean Distance, Average Accuracy: 0.9275\n",
      "With 10 neighbors, Euclidean Distance, Average Accuracy: 0.9143\n",
      "With 11 neighbors, Euclidean Distance, Average Accuracy: 0.9143\n",
      "With 12 neighbors, Euclidean Distance, Average Accuracy: 0.9187\n",
      "With 13 neighbors, Euclidean Distance, Average Accuracy: 0.9143\n",
      "With 14 neighbors, Euclidean Distance, Average Accuracy: 0.9143\n",
      "With 15 neighbors, Euclidean Distance, Average Accuracy: 0.9143\n",
      "With 16 neighbors, Euclidean Distance, Average Accuracy: 0.9209\n",
      "With 17 neighbors, Euclidean Distance, Average Accuracy: 0.9187\n",
      "With 18 neighbors, Euclidean Distance, Average Accuracy: 0.9165\n",
      "With 19 neighbors, Euclidean Distance, Average Accuracy: 0.9187\n",
      "With 20 neighbors, Euclidean Distance, Average Accuracy: 0.9187\n",
      "With 1 neighbors, Manhattan Distance, Average Accuracy: 0.9363\n",
      "With 2 neighbors, Manhattan Distance, Average Accuracy: 0.9187\n",
      "With 3 neighbors, Manhattan Distance, Average Accuracy: 0.9253\n",
      "With 4 neighbors, Manhattan Distance, Average Accuracy: 0.9275\n",
      "With 5 neighbors, Manhattan Distance, Average Accuracy: 0.9363\n",
      "With 6 neighbors, Manhattan Distance, Average Accuracy: 0.9275\n",
      "With 7 neighbors, Manhattan Distance, Average Accuracy: 0.9363\n",
      "With 8 neighbors, Manhattan Distance, Average Accuracy: 0.9275\n",
      "With 9 neighbors, Manhattan Distance, Average Accuracy: 0.9341\n",
      "With 10 neighbors, Manhattan Distance, Average Accuracy: 0.9253\n",
      "With 11 neighbors, Manhattan Distance, Average Accuracy: 0.9275\n",
      "With 12 neighbors, Manhattan Distance, Average Accuracy: 0.9253\n",
      "With 13 neighbors, Manhattan Distance, Average Accuracy: 0.9253\n",
      "With 14 neighbors, Manhattan Distance, Average Accuracy: 0.9253\n",
      "With 15 neighbors, Manhattan Distance, Average Accuracy: 0.9253\n",
      "With 16 neighbors, Manhattan Distance, Average Accuracy: 0.9231\n",
      "With 17 neighbors, Manhattan Distance, Average Accuracy: 0.9209\n",
      "With 18 neighbors, Manhattan Distance, Average Accuracy: 0.9209\n",
      "With 19 neighbors, Manhattan Distance, Average Accuracy: 0.9187\n",
      "With 20 neighbors, Manhattan Distance, Average Accuracy: 0.9209\n",
      "Test Accuracy with best parameters (Distance: Manhattan, Neighbors: 1): 0.9298\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# List of n_neighbors values to experiment with\n",
    "n_neighbors_values = list(range(1, 21))  # Adjust or expand as needed\n",
    "distances = ['euclidean', 'manhattan']\n",
    "\n",
    "best_accuracy = 0\n",
    "best_n_neighbors = None\n",
    "best_distance = None\n",
    "\n",
    "for distance in distances:\n",
    "    for n_neighbors in n_neighbors_values:\n",
    "        # Create K-Nearest Neighbors classifier\n",
    "        knn = KNeighborsClassifier(n_neighbors=n_neighbors, metric=distance)\n",
    "        \n",
    "        # Perform cross-validation on the training data\n",
    "        scores = cross_val_score(knn, X_train, y_train, cv=5)\n",
    "        \n",
    "        # Calculate average accuracy across folds\n",
    "        avg_accuracy = scores.mean()\n",
    "        \n",
    "        print(f\"With {n_neighbors} neighbors, {distance.capitalize()} Distance, Average Accuracy: {avg_accuracy:.4f}\")\n",
    "        \n",
    "        # Check if this model is the best so far\n",
    "        if avg_accuracy > best_accuracy:\n",
    "            best_accuracy = avg_accuracy\n",
    "            best_n_neighbors = n_neighbors\n",
    "            best_distance = distance\n",
    "\n",
    "# Now, train the model on the full training set with the best parameters found\n",
    "best_knn = KNeighborsClassifier(n_neighbors=best_n_neighbors, metric=best_distance)\n",
    "best_knn.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on the separate test set\n",
    "test_accuracy = best_knn.score(X_test, y_test)\n",
    "print(f\"Test Accuracy with best parameters (Distance: {best_distance.capitalize()}, Neighbors: {best_n_neighbors}): {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "d78de0a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With min_samples_leaf=1, Average Accuracy: 0.9165\n",
      "With min_samples_leaf=5, Average Accuracy: 0.9319\n",
      "With min_samples_leaf=10, Average Accuracy: 0.9143\n",
      "With min_samples_leaf=20, Average Accuracy: 0.9077\n",
      "With min_samples_leaf=25, Average Accuracy: 0.9077\n",
      "With min_samples_leaf=30, Average Accuracy: 0.9033\n",
      "Test Accuracy with best min_samples_leaf=5: 0.9561\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# List of min_samples_leaf values to experiment with\n",
    "min_samples_leaf_values = [1, 5, 10, 20, 25, 30]  # Adjust as needed\n",
    "\n",
    "best_accuracy = 0\n",
    "best_min_samples_leaf = None\n",
    "\n",
    "for min_samples_leaf in min_samples_leaf_values:\n",
    "    # Create Decision Tree Classifier\n",
    "    clf = DecisionTreeClassifier(min_samples_leaf=min_samples_leaf, random_state=42)\n",
    "    \n",
    "    # Perform cross-validation on the training data\n",
    "    scores = cross_val_score(clf, X_train, y_train, cv=5)\n",
    "    \n",
    "    # Calculate average accuracy across folds\n",
    "    avg_accuracy = scores.mean()\n",
    "    \n",
    "    print(f\"With min_samples_leaf={min_samples_leaf}, Average Accuracy: {avg_accuracy:.4f}\")\n",
    "    \n",
    "    # Check if this model is the best so far\n",
    "    if avg_accuracy > best_accuracy:\n",
    "        best_accuracy = avg_accuracy\n",
    "        best_min_samples_leaf = min_samples_leaf\n",
    "\n",
    "# Now, train the model on the full training set with the best parameter found\n",
    "best_clf = DecisionTreeClassifier(min_samples_leaf=best_min_samples_leaf, random_state=42)\n",
    "best_clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on the separate test set\n",
    "test_accuracy = best_clf.score(X_test, y_test)\n",
    "print(f\"Test Accuracy with best min_samples_leaf={best_min_samples_leaf}: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "df00a819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With max_features=0.3, Average Accuracy: 0.9560\n",
      "With max_features=0.4, Average Accuracy: 0.9582\n",
      "With max_features=0.5, Average Accuracy: 0.9538\n",
      "With max_features=0.6, Average Accuracy: 0.9560\n",
      "With max_features=0.7, Average Accuracy: 0.9538\n",
      "Test Accuracy with best max_features=0.4: 0.9561\n"
     ]
    }
   ],
   "source": [
    "# List of max_features values to experiment with\n",
    "max_features_values = [ 0.3, 0.4, 0.5, 0.6, 0.7]  # Adjust or expand as needed\n",
    "\n",
    "best_accuracy = 0\n",
    "best_max_features = None\n",
    "\n",
    "for max_feature in max_features_values:\n",
    "    # Create Random Forest Classifier\n",
    "    rf = RandomForestClassifier(n_estimators=500, min_samples_leaf=5, max_features=max_feature, random_state=42)\n",
    "    \n",
    "    # Perform cross-validation on the training data\n",
    "    scores = cross_val_score(rf, X_train, y_train, cv=5)\n",
    "    \n",
    "    # Calculate average accuracy across folds\n",
    "    avg_accuracy = scores.mean()\n",
    "    \n",
    "    print(f\"With max_features={max_feature}, Average Accuracy: {avg_accuracy:.4f}\")\n",
    "    \n",
    "    # Check if this model is the best so far\n",
    "    if avg_accuracy > best_accuracy:\n",
    "        best_accuracy = avg_accuracy\n",
    "        best_max_features = max_feature\n",
    "\n",
    "# Train the model on the full training set with the best max_features found\n",
    "best_rf = RandomForestClassifier(n_estimators=500, min_samples_leaf=5, max_features=best_max_features, random_state=42)\n",
    "best_rf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on the separate test set\n",
    "test_accuracy = best_rf.score(X_test, y_test)\n",
    "print(f\"Test Accuracy with best max_features={best_max_features}: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "b3b7a506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 200}\n",
      "Best Cross-validation Accuracy: 0.9648351648351647\n",
      "Test Set Accuracy: 0.956140350877193\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the parameter grid for tuning\n",
    "param_grid = {\n",
    "    'learning_rate': [0.1, 0.2, 0.3],  # Learning rate values to try\n",
    "    'n_estimators': [100, 200],  # Number of trees to try\n",
    "    'max_depth': [3, 5, 7]  # Depth of trees to try\n",
    "}\n",
    "\n",
    "#since the data is large, code runs for too long when i tried for 5 alternatives each\n",
    "\n",
    "# Create the Gradient Boosting Classifier\n",
    "gbt = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Use GridSearchCV to search for the best parameters\n",
    "grid_search = GridSearchCV(gbt, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)  # Using the training set for tuning\n",
    "\n",
    "# Get the best parameters and the best score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best Cross-validation Accuracy:\", best_score)\n",
    "\n",
    "# Get the best model based on the best parameters\n",
    "best_gbt = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "test_accuracy = best_gbt.score(X_test, y_test)\n",
    "print(\"Test Set Accuracy:\", test_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d137b9ae",
   "metadata": {},
   "source": [
    "Although train accuracies are different, test results of RF, DT and GBT are the same and higher than NN method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd6732e",
   "metadata": {},
   "source": [
    "### DATA5 Personality classification Data: 16 Personalities\n",
    "Data link: https://www.kaggle.com/datasets/anshulmehtakaggl/60k-responses-of-16-personalities-test-mbt?select=16P.csv\n",
    "Data size: 60,000 x62\n",
    "It has different questions asked in the 16P personality test (variables/columns); answers are like : Fully Agree: 3\n",
    "Partially Agree: 2\n",
    "Slightly Agree: 1\n",
    "neutral -> 0\n",
    "Slightly disagree: -1\n",
    "Partially disagree: -2\n",
    "Fully disagree: -3\n",
    "I encoded the 16 personalities as 0, 1, 2.., 16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "ad4b677b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Replace 'file_path' with the path to your CSV file\n",
    "file_path = r'C:\\Users\\DELL\\Downloads\\personality.csv'\n",
    "# Read the CSV file into a DataFrame\n",
    "data = pd.read_csv(file_path, encoding='latin-1')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "ea1bafc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Response Id</th>\n",
       "      <th>You regularly make new friends.</th>\n",
       "      <th>You spend a lot of your free time exploring various random topics that pique your interest</th>\n",
       "      <th>Seeing other people cry can easily make you feel like you want to cry too</th>\n",
       "      <th>You often make a backup plan for a backup plan.</th>\n",
       "      <th>You usually stay calm, even under a lot of pressure</th>\n",
       "      <th>At social events, you rarely try to introduce yourself to new people and mostly talk to the ones you already know</th>\n",
       "      <th>You prefer to completely finish one project before starting another.</th>\n",
       "      <th>You are very sentimental.</th>\n",
       "      <th>You like to use organizing tools like schedules and lists.</th>\n",
       "      <th>...</th>\n",
       "      <th>You believe that pondering abstract philosophical questions is a waste of time.</th>\n",
       "      <th>You feel more drawn to places with busy, bustling atmospheres than quiet, intimate places.</th>\n",
       "      <th>You know at first glance how someone is feeling.</th>\n",
       "      <th>You often feel overwhelmed.</th>\n",
       "      <th>You complete things methodically without skipping over any steps.</th>\n",
       "      <th>You are very intrigued by things labeled as controversial.</th>\n",
       "      <th>You would pass along a good opportunity if you thought someone else needed it more.</th>\n",
       "      <th>You struggle with deadlines.</th>\n",
       "      <th>You feel confident that things will work out for you.</th>\n",
       "      <th>Personality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ENFP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>-3</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>3</td>\n",
       "      <td>ISFP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>INFJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>-1</td>\n",
       "      <td>ISTP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>ENFJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59994</th>\n",
       "      <td>59994</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-3</td>\n",
       "      <td>-2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>INFJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59995</th>\n",
       "      <td>59995</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>-3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>ESTP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59996</th>\n",
       "      <td>59996</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>ISTP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59997</th>\n",
       "      <td>59997</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>ISTJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59998</th>\n",
       "      <td>59998</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>INFJ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>59999 rows × 62 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Response Id  You regularly make new friends.  \\\n",
       "0                0                                0   \n",
       "1                1                                0   \n",
       "2                2                                0   \n",
       "3                3                                0   \n",
       "4                4                                0   \n",
       "...            ...                              ...   \n",
       "59994        59994                                0   \n",
       "59995        59995                                0   \n",
       "59996        59996                                0   \n",
       "59997        59997                                0   \n",
       "59998        59998                                0   \n",
       "\n",
       "       You spend a lot of your free time exploring various random topics that pique your interest  \\\n",
       "0                                                      0                                            \n",
       "1                                                      0                                            \n",
       "2                                                      0                                            \n",
       "3                                                     -1                                            \n",
       "4                                                      0                                            \n",
       "...                                                  ...                                            \n",
       "59994                                                 -1                                            \n",
       "59995                                                  0                                            \n",
       "59996                                                  0                                            \n",
       "59997                                                  0                                            \n",
       "59998                                                  0                                            \n",
       "\n",
       "       Seeing other people cry can easily make you feel like you want to cry too  \\\n",
       "0                                                      0                           \n",
       "1                                                     -2                           \n",
       "2                                                      2                           \n",
       "3                                                      3                           \n",
       "4                                                     -1                           \n",
       "...                                                  ...                           \n",
       "59994                                                  0                           \n",
       "59995                                                 -2                           \n",
       "59996                                                  1                           \n",
       "59997                                                  1                           \n",
       "59998                                                  2                           \n",
       "\n",
       "       You often make a backup plan for a backup plan.  \\\n",
       "0                                                    0   \n",
       "1                                                   -3   \n",
       "2                                                    0   \n",
       "3                                                   -1   \n",
       "4                                                    0   \n",
       "...                                                ...   \n",
       "59994                                               -3   \n",
       "59995                                                1   \n",
       "59996                                                0   \n",
       "59997                                               -1   \n",
       "59998                                                0   \n",
       "\n",
       "       You usually stay calm, even under a lot of pressure  \\\n",
       "0                                                      0     \n",
       "1                                                     -1     \n",
       "2                                                     -1     \n",
       "3                                                      0     \n",
       "4                                                      2     \n",
       "...                                                  ...     \n",
       "59994                                                 -2     \n",
       "59995                                                  3     \n",
       "59996                                                 -2     \n",
       "59997                                                 -2     \n",
       "59998                                                 -1     \n",
       "\n",
       "       At social events, you rarely try to introduce yourself to new people and mostly talk to the ones you already know  \\\n",
       "0                                                      1                                                                   \n",
       "1                                                      2                                                                   \n",
       "2                                                      2                                                                   \n",
       "3                                                      0                                                                   \n",
       "4                                                     -1                                                                   \n",
       "...                                                  ...                                                                   \n",
       "59994                                                  2                                                                   \n",
       "59995                                                  0                                                                   \n",
       "59996                                                 -2                                                                   \n",
       "59997                                                 -1                                                                   \n",
       "59998                                                  0                                                                   \n",
       "\n",
       "       You prefer to completely finish one project before starting another.  \\\n",
       "0                                                      1                      \n",
       "1                                                     -2                      \n",
       "2                                                      0                      \n",
       "3                                                     -2                      \n",
       "4                                                     -2                      \n",
       "...                                                  ...                      \n",
       "59994                                                  1                      \n",
       "59995                                                 -3                      \n",
       "59996                                                  0                      \n",
       "59997                                                  2                      \n",
       "59998                                                  0                      \n",
       "\n",
       "       You are very sentimental.  \\\n",
       "0                              0   \n",
       "1                              0   \n",
       "2                              0   \n",
       "3                              0   \n",
       "4                              0   \n",
       "...                          ...   \n",
       "59994                          0   \n",
       "59995                          0   \n",
       "59996                          0   \n",
       "59997                          0   \n",
       "59998                          0   \n",
       "\n",
       "       You like to use organizing tools like schedules and lists.  ...  \\\n",
       "0                                                      0           ...   \n",
       "1                                                      3           ...   \n",
       "2                                                      1           ...   \n",
       "3                                                     -2           ...   \n",
       "4                                                      1           ...   \n",
       "...                                                  ...           ...   \n",
       "59994                                                  1           ...   \n",
       "59995                                                  2           ...   \n",
       "59996                                                 -1           ...   \n",
       "59997                                                  1           ...   \n",
       "59998                                                  2           ...   \n",
       "\n",
       "       You believe that pondering abstract philosophical questions is a waste of time.  \\\n",
       "0                                                      0                                 \n",
       "1                                                      0                                 \n",
       "2                                                      0                                 \n",
       "3                                                      0                                 \n",
       "4                                                      0                                 \n",
       "...                                                  ...                                 \n",
       "59994                                                  0                                 \n",
       "59995                                                  0                                 \n",
       "59996                                                 -1                                 \n",
       "59997                                                  0                                 \n",
       "59998                                                  0                                 \n",
       "\n",
       "       You feel more drawn to places with busy, bustling atmospheres than quiet, intimate places.  \\\n",
       "0                                                      0                                            \n",
       "1                                                     -2                                            \n",
       "2                                                      2                                            \n",
       "3                                                      0                                            \n",
       "4                                                      1                                            \n",
       "...                                                  ...                                            \n",
       "59994                                                  2                                            \n",
       "59995                                                  1                                            \n",
       "59996                                                  2                                            \n",
       "59997                                                  2                                            \n",
       "59998                                                 -1                                            \n",
       "\n",
       "       You know at first glance how someone is feeling.  \\\n",
       "0                                                     0   \n",
       "1                                                     0   \n",
       "2                                                     0   \n",
       "3                                                    -1   \n",
       "4                                                     0   \n",
       "...                                                 ...   \n",
       "59994                                                 0   \n",
       "59995                                                 0   \n",
       "59996                                                 0   \n",
       "59997                                                 0   \n",
       "59998                                                 0   \n",
       "\n",
       "       You often feel overwhelmed.  \\\n",
       "0                               -1   \n",
       "1                                2   \n",
       "2                                2   \n",
       "3                               -1   \n",
       "4                                2   \n",
       "...                            ...   \n",
       "59994                            1   \n",
       "59995                            0   \n",
       "59996                           -3   \n",
       "59997                            2   \n",
       "59998                            3   \n",
       "\n",
       "       You complete things methodically without skipping over any steps.  \\\n",
       "0                                                      0                   \n",
       "1                                                      0                   \n",
       "2                                                     -1                   \n",
       "3                                                      0                   \n",
       "4                                                      0                   \n",
       "...                                                  ...                   \n",
       "59994                                                  0                   \n",
       "59995                                                  0                   \n",
       "59996                                                  0                   \n",
       "59997                                                  0                   \n",
       "59998                                                  1                   \n",
       "\n",
       "       You are very intrigued by things labeled as controversial.  \\\n",
       "0                                                      0            \n",
       "1                                                     -1            \n",
       "2                                                      0            \n",
       "3                                                      1            \n",
       "4                                                      1            \n",
       "...                                                  ...            \n",
       "59994                                                  0            \n",
       "59995                                                  0            \n",
       "59996                                                  1            \n",
       "59997                                                  0            \n",
       "59998                                                  0            \n",
       "\n",
       "       You would pass along a good opportunity if you thought someone else needed it more.  \\\n",
       "0                                                      0                                     \n",
       "1                                                     -1                                     \n",
       "2                                                      1                                     \n",
       "3                                                      0                                     \n",
       "4                                                     -1                                     \n",
       "...                                                  ...                                     \n",
       "59994                                                  2                                     \n",
       "59995                                                  2                                     \n",
       "59996                                                 -1                                     \n",
       "59997                                                  0                                     \n",
       "59998                                                  1                                     \n",
       "\n",
       "       You struggle with deadlines.  \\\n",
       "0                                 0   \n",
       "1                                -1   \n",
       "2                                 2   \n",
       "3                                -2   \n",
       "4                                 2   \n",
       "...                             ...   \n",
       "59994                             3   \n",
       "59995                             0   \n",
       "59996                             0   \n",
       "59997                             1   \n",
       "59998                             0   \n",
       "\n",
       "       You feel confident that things will work out for you.  Personality  \n",
       "0                                                      0             ENFP  \n",
       "1                                                      3             ISFP  \n",
       "2                                                      1             INFJ  \n",
       "3                                                     -1             ISTP  \n",
       "4                                                     -1             ENFJ  \n",
       "...                                                  ...              ...  \n",
       "59994                                                  3             INFJ  \n",
       "59995                                                 -2             ESTP  \n",
       "59996                                                 -1             ISTP  \n",
       "59997                                                  0             ISTJ  \n",
       "59998                                                 -1             INFJ  \n",
       "\n",
       "[59999 rows x 62 columns]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "592f7c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming your data is loaded into a DataFrame named 'data'\n",
    "# Extracting features (X) and target (y)\n",
    "X = data.drop('Personality', axis=1)  # Replace 'target_column_name' with your target column\n",
    "y = data['Personality']  # Replace 'target_column_name' with your target column\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "# Assuming 'y' contains your categorical labels 'M' and 'B'\n",
    "# Replace 'y' with your actual target variable\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "y = np.ravel(y)\n",
    "\n",
    "# Fit LabelEncoder and transform the target column\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "# Now 'y_encoded' contains the encoded values (0 and 1)\n",
    "\n",
    "# Split the data into training and test sets (adjust test_size as needed)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "332f9696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With 5 neighbors, Euclidean Distance, Average Accuracy: 0.1218\n",
      "With 10 neighbors, Euclidean Distance, Average Accuracy: 0.0948\n",
      "With 15 neighbors, Euclidean Distance, Average Accuracy: 0.0813\n",
      "With 20 neighbors, Euclidean Distance, Average Accuracy: 0.0760\n",
      "With 25 neighbors, Euclidean Distance, Average Accuracy: 0.0721\n",
      "With 5 neighbors, Manhattan Distance, Average Accuracy: 0.2282\n",
      "With 10 neighbors, Manhattan Distance, Average Accuracy: 0.1969\n",
      "With 15 neighbors, Manhattan Distance, Average Accuracy: 0.1751\n",
      "With 20 neighbors, Manhattan Distance, Average Accuracy: 0.1612\n",
      "With 25 neighbors, Manhattan Distance, Average Accuracy: 0.1478\n",
      "Test Accuracy with best parameters (Distance: Manhattan, Neighbors: 5): 0.2572\n"
     ]
    }
   ],
   "source": [
    "# List of n_neighbors values to experiment with\n",
    "n_neighbors_values = [5 ,10 ,15, 20, 25]  # Adjust or expand as needed\n",
    "distances = ['euclidean', 'manhattan']\n",
    "\n",
    "best_accuracy = 0\n",
    "best_n_neighbors = None\n",
    "best_distance = None\n",
    "\n",
    "for distance in distances:\n",
    "    for n_neighbors in n_neighbors_values:\n",
    "        # Create K-Nearest Neighbors classifier\n",
    "        knn = KNeighborsClassifier(n_neighbors=n_neighbors, metric=distance)\n",
    "        \n",
    "        # Perform cross-validation on the training data\n",
    "        scores = cross_val_score(knn, X_train, y_train, cv=5)\n",
    "        \n",
    "        # Calculate average accuracy across folds\n",
    "        avg_accuracy = scores.mean()\n",
    "        \n",
    "        print(f\"With {n_neighbors} neighbors, {distance.capitalize()} Distance, Average Accuracy: {avg_accuracy:.4f}\")\n",
    "        \n",
    "        # Check if this model is the best so far\n",
    "        if avg_accuracy > best_accuracy:\n",
    "            best_accuracy = avg_accuracy\n",
    "            best_n_neighbors = n_neighbors\n",
    "            best_distance = distance\n",
    "\n",
    "# Now, train the model on the full training set with the best parameters found\n",
    "best_knn = KNeighborsClassifier(n_neighbors=best_n_neighbors, metric=best_distance)\n",
    "best_knn.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on the separate test set\n",
    "test_accuracy = best_knn.score(X_test, y_test)\n",
    "print(f\"Test Accuracy with best parameters (Distance: {best_distance.capitalize()}, Neighbors: {best_n_neighbors}): {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "ef072afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With min_samples_leaf=1, Average Accuracy: 0.6262\n",
      "With min_samples_leaf=5, Average Accuracy: 0.6330\n",
      "With min_samples_leaf=10, Average Accuracy: 0.6217\n",
      "With min_samples_leaf=20, Average Accuracy: 0.5997\n",
      "With min_samples_leaf=25, Average Accuracy: 0.5897\n",
      "With min_samples_leaf=30, Average Accuracy: 0.5803\n",
      "Test Accuracy with best min_samples_leaf=5: 0.6523\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# List of min_samples_leaf values to experiment with\n",
    "min_samples_leaf_values = [1, 5, 10, 20, 25, 30]  # Adjust as needed\n",
    "\n",
    "best_accuracy = 0\n",
    "best_min_samples_leaf = None\n",
    "\n",
    "for min_samples_leaf in min_samples_leaf_values:\n",
    "    # Create Decision Tree Classifier\n",
    "    clf = DecisionTreeClassifier(min_samples_leaf=min_samples_leaf, random_state=42)\n",
    "    \n",
    "    # Perform cross-validation on the training data\n",
    "    scores = cross_val_score(clf, X_train, y_train, cv=5)\n",
    "    \n",
    "    # Calculate average accuracy across folds\n",
    "    avg_accuracy = scores.mean()\n",
    "    \n",
    "    print(f\"With min_samples_leaf={min_samples_leaf}, Average Accuracy: {avg_accuracy:.4f}\")\n",
    "    \n",
    "    # Check if this model is the best so far\n",
    "    if avg_accuracy > best_accuracy:\n",
    "        best_accuracy = avg_accuracy\n",
    "        best_min_samples_leaf = min_samples_leaf\n",
    "\n",
    "# Now, train the model on the full training set with the best parameter found\n",
    "best_clf = DecisionTreeClassifier(min_samples_leaf=best_min_samples_leaf, random_state=42)\n",
    "best_clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on the separate test set\n",
    "test_accuracy = best_clf.score(X_test, y_test)\n",
    "print(f\"Test Accuracy with best min_samples_leaf={best_min_samples_leaf}: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "200d1cd1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With max_features=0.3, Average Accuracy: 0.9625\n",
      "With max_features=0.4, Average Accuracy: 0.9589\n",
      "With max_features=0.5, Average Accuracy: 0.9544\n",
      "With max_features=0.6, Average Accuracy: 0.9508\n",
      "With max_features=0.7, Average Accuracy: 0.9455\n",
      "Test Accuracy with best max_features=0.3: 0.9656\n"
     ]
    }
   ],
   "source": [
    "# List of max_features values to experiment with\n",
    "max_features_values = [ 0.3, 0.4, 0.5, 0.6, 0.7]  # Adjust or expand as needed\n",
    "\n",
    "best_accuracy = 0\n",
    "best_max_features = None\n",
    "\n",
    "for max_feature in max_features_values:\n",
    "    # Create Random Forest Classifier\n",
    "    rf = RandomForestClassifier(n_estimators=500, min_samples_leaf=5, max_features=max_feature, random_state=42)\n",
    "    \n",
    "    # Perform cross-validation on the training data\n",
    "    scores = cross_val_score(rf, X_train, y_train, cv=5)\n",
    "    \n",
    "    # Calculate average accuracy across folds\n",
    "    avg_accuracy = scores.mean()\n",
    "    \n",
    "    print(f\"With max_features={max_feature}, Average Accuracy: {avg_accuracy:.4f}\")\n",
    "    \n",
    "    # Check if this model is the best so far\n",
    "    if avg_accuracy > best_accuracy:\n",
    "        best_accuracy = avg_accuracy\n",
    "        best_max_features = max_feature\n",
    "\n",
    "# Train the model on the full training set with the best max_features found\n",
    "best_rf = RandomForestClassifier(n_estimators=500, min_samples_leaf=5, max_features=best_max_features, random_state=42)\n",
    "best_rf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on the separate test set\n",
    "test_accuracy = best_rf.score(X_test, y_test)\n",
    "print(f\"Test Accuracy with best max_features={best_max_features}: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c19696",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the parameter grid for tuning\n",
    "param_grid = {\n",
    "    'learning_rate': [0.1, 0.2, 0.3],  # Learning rate values to try\n",
    "    'n_estimators': [ 100, 200],  # Number of trees to try\n",
    "    'max_depth': [3, 5 , 7]  # Depth of trees to try\n",
    "}\n",
    "\n",
    "#since the data is large, code runs for too long when i tried for 5 alternatives each\n",
    "\n",
    "# Create the Gradient Boosting Classifier\n",
    "gbt = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Use GridSearchCV to search for the best parameters\n",
    "grid_search = GridSearchCV(gbt, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)  # Using the training set for tuning\n",
    "\n",
    "# Get the best parameters and the best score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best Cross-validation Accuracy:\", best_score)\n",
    "\n",
    "# Get the best model based on the best parameters\n",
    "best_gbt = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "test_accuracy = best_gbt.score(X_test, y_test)\n",
    "print(\"Test Set Accuracy:\", test_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679b19fe",
   "metadata": {},
   "source": [
    "NN has very low accuracy results and it is not suitable for this data. It might be said that some questions are more decisive and they are not weighed the same way in the personality test, so different answers to those questions gives very different results.  GBT took very long(even when i considered no alternatives), so i couldn't apply this method to this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfac439",
   "metadata": {},
   "source": [
    "##ALL RESULTS:\n",
    "    \n",
    "    \n",
    "1.\n",
    "\n",
    "Best neighbors for Euclidean distance: 12, Train set accuracy:0.7288, Test Set Accuracy: 0.7350\n",
    "Best neighbors for Manhattan distance: 12,Train set accuracy:0.7288, Test Set Accuracy: 0.7350\n",
    "Best min_samples_leaf=20, Train set accuracy:0.7112 Test Set Accuracy: 0.7200\n",
    "Best max_features=0.4, Train set accuracy: 0.7400, Test Set Accuracy: 0.7450\n",
    "Best Parameters: {'classifier__learning_rate': 0.01, 'classifier__max_depth': 5, 'classifier__n_estimators': 200}\n",
    "Best Cross-Validation Score: 0.7350\n",
    "Test Set Score: 0.7300\n",
    "Best RF\n",
    "\n",
    "2.\n",
    "With 1 neighbors, Euclidean Distance, Average Accuracy: 0.9928\n",
    "Test Accuracy with best parameters (Distance: Euclidean, Neighbors: 1): 0.9936\n",
    "With min_samples_leaf=1, Average Accuracy: 0.9904\n",
    "Test Accuracy with best min_samples_leaf=1: 0.9920\n",
    "With max_features=0.5, Average Accuracy: 0.9936\n",
    "Test Accuracy with best max_features=0.5: 0.9904\n",
    "Best Parameters: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 200}\n",
    "Best Cross-validation Accuracy: 0.7858153733048849\n",
    "Test Set Accuracy: 0.7694915254237288\n",
    "Best NN\n",
    "\n",
    "\n",
    "3.\n",
    "With 18 neighbors, Manhattan Distance, Average Accuracy: 0.6626\n",
    "Test Accuracy with best parameters (Distance: Manhattan, Neighbors: 18): 0.6418\n",
    "With min_samples_leaf=25, Average Accuracy: 0.7550\n",
    "Test Accuracy with best min_samples_leaf=25: 0.7311\n",
    "With max_features=0.7, Average Accuracy: 0.7754\n",
    "Test Accuracy with best max_features=0.7: 0.7571\n",
    "Best Parameters: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 200}\n",
    "Best Cross-validation Accuracy: 0.7858153733048849\n",
    "Test Set Accuracy: 0.7694915254237288\n",
    "\n",
    "Best GBT\n",
    "\n",
    "4.\n",
    "With 1 neighbors, Manhattan Distance, Average Accuracy: 0.9363\n",
    "Test Accuracy with best parameters (Distance: Manhattan, Neighbors: 1): 0.9298\n",
    "With min_samples_leaf=5, Average Accuracy: 0.9319\n",
    "Test Accuracy with best min_samples_leaf=5: 0.9561\n",
    "With max_features=0.4, Average Accuracy: 0.9582\n",
    "Test Accuracy with best max_features=0.4: 0.9561\n",
    "Best Parameters: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 200}\n",
    "Best Cross-validation Accuracy: 0.9648351648351647\n",
    "Test Set Accuracy: 0.956140350877193\n",
    "\n",
    "Best: RF DT and GBT\n",
    "\n",
    "5.\n",
    "With 5 neighbors, Manhattan Distance, Average Accuracy: 0.2282\n",
    "Test Accuracy with best parameters (Distance: Manhattan, Neighbors: 5): 0.2572\n",
    "With min_samples_leaf=5, Average Accuracy: 0.6330\n",
    "Test Accuracy with best min_samples_leaf=5: 0.6523\n",
    "With max_features=0.3, Average Accuracy: 0.9625\n",
    "Test Accuracy with best max_features=0.3: 0.9656\n",
    "\n",
    "Best RF\n",
    "\n",
    "\n",
    "2nd and 4th datasets seem to have the overall best results. 2nd dataset consists mechanical data and 4th has health data. Both of them are measured and objective values. Thus, they have good results with all methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73a4c7e",
   "metadata": {},
   "source": [
    "## APPENDIX\n",
    "I utilized ChatGPT.\n",
    "\n",
    "ChatHistory:\n",
    "https://chat.openai.com/share/dae04490-4846-4d80-a0a2-bcb49a3f0921"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f1fae4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
